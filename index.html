<!DOCTYPE html>
<!--[if IE 8 ]><html class="no-js oldie ie8" lang="en"> <![endif]-->
<!--[if IE 9 ]><html class="no-js oldie ie9" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html class="no-js" lang="en"> <!--<![endif]-->
<head>

   <!--- basic page needs
   ================================================== -->
   <meta charset="utf-8">
	<title>Alireza Kazemipour</title>
	<meta name="description" content="">  
	<meta name="author" content="">

   <!-- mobile specific metas
   ================================================== -->
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

 	<!-- CSS
   ================================================== -->
   <link rel="stylesheet" href="css/base.css">  
   <link rel="stylesheet" href="css/main.css">
   <link rel="stylesheet" href="css/vendor.css">     

   <!-- script
   ================================================== -->   
	<script src="js/modernizr.js"></script>
	<script src="js/pace.min.js"></script>

   <!-- favicons
	================================================== -->
	<link rel="icon" type="image/png" href="images/favicon.png">

</head>

<body id="top">

	<!-- header 
   ================================================== -->
   <header>   	
   	<div class="row">

   		<div class="top-bar">
   			<a class="menu-toggle" href="#"><span>Menu</span></a>

	   		<div class="logo">
		         <a href="index.html">Contents</a>
		      </div>		      

		   	<nav id="main-nav-wrap">
					<ul class="main-navigation">
						<li class="current"><a class="smoothscroll"  href="#intro" title="">Home</a></li>
						<li><a class="smoothscroll"  href="#about" title="">About</a></li>
						<li><a class="smoothscroll"  href="#resume" title="">Resume</a></li>
						<li><a class="smoothscroll"  href="#contact" title="">Contact</a></li>	
					</ul>
				</nav>    		
   		</div> <!-- /top-bar --> 
   		
   	</div> <!-- /row --> 		
   </header> <!-- /header -->

	<!-- intro section
   ================================================== -->
   <section id="intro">   

   	<div class="intro-overlay"></div>	

   	<div class="intro-content">
   		<div class="row">

   			<div class="col-twelve">

	   			<h5>Hi, welcome to my homepage!</h5>
	   			<h1>I'm Alireza.</h1>

	   			<p class="intro-position">
	   				<span>Electrical-Control Engineering student at K. N. Toosi University of Technology</span>
	   				<!-- <span>UI/UX Designer</span>  -->
	   			</p>

	   			<a class="button stroke smoothscroll" href="#about" title="">More About Me</a>

	   		</div>  
   			
   		</div>   		 		
   	</div> <!-- /intro-content --> 

   	<ul class="intro-social">        
         <li><a href="https://www.linkedin.com/in/alireza-kazemipour-1b7a511a0/" ><i class="fa fa-linkedin"></i></a></li>
         <li><a href="mailto:kazemipour.alireza@gmail.com"><i class="fa fa-envelope"></i></a></li>
         <li><a href="https://twitter.com/AR_Kazemipour"><i class="fa fa-twitter"></i></a></li>
         <li><a href="https://github.com/alirezakazemipour"><i class="fa fa-github"></i></a></li>
         <!-- <li><a href="#"><i class="fa fa-instagram"></i></a></li> -->
      </ul> <!-- /intro-social -->      	

   </section> <!-- /intro -->


   <!-- about section
   ================================================== -->
   <section id="about">  

   	<div class="row section-intro">
   		<div class="col-twelve">

   			<h5>About</h5>
   			<h1>Me in a nutshell.</h1>

   			<div class="intro-info">

   				<img src="images/profile-pic.jpg" alt="Profile Picture">

   				<p class="lead">I'm a graduate B.Sc. student from K. N. Toosi University of Technology, studied Electrical-Control Engineering meanwhile, I was and still am fascinated by A.I. thus, I tried to experience many subfields of A.I. and in the end I found (Deep)(Multi-Agent) Reinforcement Learning as my main Research Interest since its objective is as exactly as what the objective of A.I. is! &#128525;</p>
   			</div>   			

   		</div>   		
   	</div> <!-- /section-intro -->

   	<div class="row about-content">

   		<div class="col-six tab-full">

   			<h3>Profile</h3>
   			<p>I obtained my Bachelor's degree in Electrical engineering, Control field. Throughout my education, I tried to gain as much as possible experiences in Robotics and Artificial intelligence. I worked as a research assistant in <a href="http://kn2c.aras.kntu.ac.ir/">the KN2C robotics lab</a> for more than two years in the UAV division as a member of A.I. and the Vision group mainly focused on coding with OpenCV library, ROS framework, and embedded systems. During the last year of my study, I officially pursued the path of A.I. and I could implement some brilliant papers in this elegant field.</p>

   			<ul class="info-list">
   				<li>
   					<strong>Research Interests:</strong>
   					<ul>
   						<li>
   					<span>(Deep)(Multi-Agent) Reinforcement Learning</span>
   						</li>
   						<li>
   					<span>Deep Learning</span>
   						</li>
   					    <li>
   					<span>Computer Vision</span>
   						</li>
   					   	<li>
   					<span>Robotics</span>
   						</li>
   					   	<li>
   					<span>Self-Play</span>
   						</li>
   					   	<li>
   					<span>Control Theory</span>
   						</li>
   					</ul>
   				</li>	
   				<li>
   					<strong>Full Name:</strong>
   					<span>Alireza Kazemipour</span>
   				</li>
   				<li>
   					<strong>Birth Date:</strong>
   					<span>September 26, 1996</span>
   				</li>
   			</ul> <!-- /info-list -->

   		</div>

   		<div class="col-six tab-full">

   			<h3>Skills</h3>

				<ul class="skill-list">
   				
   					<strong style="color: black;">Programming Languages:</strong>
   					<ul>
   						<li>
   					<span>Python</span>
   						</li>
   						<li>
   					<span>C/C++</span>
   						</li>
   					    <li>
   					<span>Bash</span>
   						</li>
   					   	<li>
   					<span>VHDL</span>
   						</li>
   					   	<li>
   					<span>Java (Familiar)</span>
   						</li>
   					   	<li>
   					<span>Assemble (Familiar)</span>
   						</li>
   					</ul>

   					<strong style="color: black;">Libraries:</strong>
   					<ul>
   						<li>
   					<span>PyTorch</span>
   						</li>
   						<li>
   					<span>TensorFlow</span>
   						</li>
   					    <li>
   					<span>Keras</span>
   						</li>
   					   	<li>
   					<span>Numpy</span>
   						</li>
   					   	<li>
   					<span>Scikit-learn</span>
   						</li>
   					   	<li>
   					<span>Gym</span>
   						</li>
   					   	<li>
   					<span>OpenCV</span>
   						</li>   						
   					</ul>					

   					<strong style="color: black;">Frameworks:</strong>
   					<ul>
   						<li>
   					<span>Qt</span>
   						</li>
   						<li>
   					<span>ROS (Robot Operating System)</span>
   						</li>
   					</ul>

   					<strong style="color: black;">Engineering Softwares:</strong>
   					<ul>
   						<li>
   					<span>MatLab and Simulink</span>
   						</li>
   						<li>
   					<span>Keil μVision</span>
   						</li>
   						<li>
   					<span>Altium Designer</span>
   						</li>
   					    <li>
   					<span>ISE - Xilinx</span>
   						</li>
   					</ul>

   					<strong style="color: black;">Operating Systems:</strong>
   					<ul>
   						<li>
   					<span>Linux (Ubuntu)</span>
   						</li>
   					</ul>

   					<strong style="color: black;">Version Control Systems:</strong>
   					<ul>
   						<li>
   					<span>Git</span>
   						</li>
   					</ul>
				</ul> <!-- /skill-bars -->		

   		</div>

   	</div>

   	<div class="row button-section">
   		<div class="col-twelve">
   			<!-- <a href="#contact" title="Hire Me" class="button stroke smoothscroll">Hire Me</a> -->
   			<a href="pdfs/CV.pdf" title="Download CV" class="button button-primary">Download CV</a>
   		</div>   		
   	</div>

   </section> <!-- /process-->    


   <!-- resume Section
   ================================================== -->
	<section id="resume" class="grey-section">

		<div class="row section-intro">
   		<div class="col-twelve">

   			<h5>Resume</h5>
   			<h1>More of my credentials.</h1>

   			<p class="lead">Here is the list of my Academic Records, Certificates, Awards, and Projects that I have done so far.</p>

   		</div>   		
   	</div> <!-- /section-intro--> 

   	<div class="row resume-timeline">

   		<div class="col-twelve resume-header">

   			<h2>Education</h2>

   		</div> <!-- /resume-header -->

   		<div class="col-twelve">

   			<div class="timeline-wrap">

   				<div class="timeline-block">

	   				<div class="timeline-ico">
	   					<i class="fa fa-graduation-cap"></i>
	   				</div>

	   				<div class="timeline-header">
	   					<h3>B.Sc. in Electrial (Control) Engineering</h3>
	   					<p>2015 Sep - 2020 Sep</p>
	   				</div>

	   				<div class="timeline-content">
	   					<h4>K. N. Toosi University of Technology, Tehran, Iran</h4>
	   					<p>Cumulative GPA: 3.2/4 (16.09/20)</p>
	   				</div>


	   			</div>

   				<div class="timeline-block">

	   				<div class="timeline-header">
	   					<h3>Lagnuage Proficiency</h3>
	   				</div>

	   				<div class="timeline-content">
	   					<h4>English Qualifications</h4>

						<ul>
							<li>
								<p>TOEFL (internet based):
								<br>Reading: 24
								<br>Listening: 29
								<br>Speaking: 25
								<br>Writing: 26</p>
							</li>
							<li>
								<p>GRE General:
								 <br>Analytical Writing: 3.5
								 <br>Verbal Reasoning: 154
								 <br>Quantitative Reasoning: 160</p>
							</li>
						</ul>
	   				</div>

	   				<div class="timeline-ico">
	   					<i class="fa fa-graduation-cap"></i>
	   				</div>

	   			</div> <!-- /timeline-block -->

   			</div> <!-- /timeline-wrap -->   			

   		</div> <!-- /col-twelve -->
   		
   	</div> <!-- /resume-timeline -->
   	
   	<div class="row resume-timeline">

   		<div class="col-twelve resume-header">

   			<h2>Experience</h2>

   		</div> <!-- /resume-header -->

   		<div class="col-twelve">

   			<div class="timeline-wrap">

	   			<div class="timeline-block">

	   				<div class="timeline-ico">
	   					<i class="fa fa-laptop"></i>
	   				</div>

	   			    <div class="timeline-header">
	   					<h3>Lecturer</h3>
	   					<p>March 2021</p>
	   				</div>

	   				<div class="timeline-content">
	   					<h4><a href="http://csicc2021.kntu.ac.ir/en/">26th Computer Society International Computer Conference </a></h4>
	   					<p>I presented some DeepRL recent advances and highlighted challenging points of DeepRL algorithms' implementations.
	   						[<a href="pdfs/RL_for_csicc2021.pdf">Slides</a>]
	   					</p>
	   				</div>


	   			</div> <!-- /timeline-block -->

	   			<div class="timeline-block">

	   				<div class="timeline-ico">
	   					<i class="fa fa-pencil-square-o"></i>
	   				</div>

	   				<div class="timeline-header">
	   					<h3>Teaching Assistant</h3>
	   					<p>Winter 2020</p>
	   				</div>

	   				<div class="timeline-content">
	   					<h4><a href="https://wp.kntu.ac.ir/nasihatkon/teaching/cvug/s2020/">Fundamentals of Computer Vision</a></h4>
	   					<p>Instructor: <a href="https://wp.kntu.ac.ir/nasihatkon/">Dr. Behrouz Nasihatkon</a></p>
	   				</div>

	   			</div> <!-- /timeline-block -->

   				<div class="timeline-block">

	   				<div class="timeline-ico">
	   					<i class="fa fa-pencil-square-o"></i>
	   				</div>

	   				<div class="timeline-header">
	   					<h3>Teaching Assistant</h3>
	   					<p>Fall 2019</p>
	   				</div>

	   				<div class="timeline-content">
	   					<h4>Signals and Systems</h4>
	   					<p>Instructor: <a href="https://wp.kntu.ac.ir/m.mohebbi/index.html">Dr. Maryam Mohebbi</a></p>
	   				</div>

	   			</div> <!-- /timeline-block -->

	   			<div class="timeline-block">

	   				<div class="timeline-ico">
	   					<i class="fa fa-keyboard-o"></i>
	   				</div>
					<div class="timeline-header">
	   					<h3>Research Assistant</h3>
	   					<p>2017 - 2019</p>
	   				</div>

	   				<div class="timeline-content">
	   					<h4><a href="http://kn2c.aras.kntu.ac.ir/">KN2C Robotics Lab</a></h4>
	   					<p>Supervisor: <a href="http://aras.kntu.ac.ir/taghirad/">Dr. Hamid D. Taghirad</a>
	   						<br>Member of Computer Vision and A.I group of <a href="http://kn2c.aras.kntu.ac.ir/mav/">Micro Aerial Vehicle</a> section.
	   					</p>
	   				</div>

	   			</div> <!-- /timeline-block -->

   			</div> <!-- /timeline-wrap -->   			

   		</div> <!-- /col-twelve -->
   		
   	</div> <!-- /resume-timeline -->

   	<div class="row resume-timeline">

   		<div class="col-twelve resume-header">

   			<h2>Certificates</h2>

   		</div> <!-- /resume-header -->

   		<div class="col-twelve">

   			<div class="timeline-wrap">

   				<div class="timeline-block">

	   				<div class="timeline-ico">
	   					<i class="fa fa-file-text"></i>
	   				</div>

	   				<div class="timeline-header">
	   					<h3>Stanford | ONLINE on Coursera</h3>
	   					<p>September 2021</p>
	   				</div>

	   				<div class="timeline-content">
	   					<h4>Algorithms Specialization</h4>
	   					<li>
	   					<p><a href="https://www.coursera.org/account/accomplishments/specialization/certificate/JEFZTZG65K2Q">Click here to see the certificate</a></p>
	   				</li>
	   				</div>

	   			</div>

   				<div class="timeline-block">

   					<div class="timeline-header">
	   					<h3>University of Alberta on Coursera</h3>
	   					<p>March 2021</p>
	   				</div>

	   				<div class="timeline-ico">
	   					<i class="fa fa-file-text"></i>
	   				</div>


	   				<div class="timeline-content">
	   					<h4>Reinforcement Learning Specialization</h4>
	   					<li>
	   					<p><a href="https://www.coursera.org/account/accomplishments/specialization/certificate/Z57MW7PU5UZV">Click here to see the certificate</a></p>
	   				</li>
	   			</div>

	   			<div class="timeline-block">

	   				<div class="timeline-ico">
	   					<i class="fa fa-file-text"></i>
	   				</div>

	   				<div class="timeline-header">
	   					<h3>DeepLearning.AI on Coursera</h3>
	   					<p>May 2021</p>
	   				</div>

	   				<div class="timeline-content">
	   					<h4>Improving Deep Neural Networks, Hyperparameter Tuning, Regularization, and Optimization</h4>
	   				<li>
	   					<p><a href="https://www.coursera.org/account/accomplishments/certificate/8MJUB2WM79RW">Click here to see the certificate</a></p>
	   				</li>
	   				</div>

	   			</div>

   				<div class="timeline-block">

	   				<div class="timeline-ico">
	   					<i class="fa fa-file-text"></i>
	   				</div>

	   				<div class="timeline-header">
	   					<h3>DeepLearning.AI on Coursera</h3>
	   					<p>December 2020</p>
	   				</div>

	   				<div class="timeline-content">
	   					<h4>Neural Networks and Deep Learning</h4>
	   				<li>
	   					<p><a href="https://www.coursera.org/account/accomplishments/certificate/4UFX4KDQHDNX">Click here to see the certificate</a></p>
	   				</li>
	   				</div>

	   			</div>


	   		</div>

	   	</div>


   		</div> <!-- /col-twelve -->
   		
   	</div> <!-- /resume-timeline -->
   	   	<div class="row resume-timeline">

   		<div class="col-twelve resume-header">

   			<h2>Awards</h2>

   		</div> <!-- /resume-header -->

   		<div class="col-twelve">

   			<div class="timeline-wrap">

   				<div class="timeline-block">

   					<div class="timeline-header">
	   					<h3>RoboCup Iran Open</h3>
	   					<p>April 2018</p>
	   				</div>

	   				<div class="timeline-ico">
	   					<i class="fa fa-trophy"></i>
	   				</div>


	   				<div class="timeline-content">
	   					<h4>Unmanned Aerial Vehicle (UAV) league</h4>
	   					<li>
	   					<p><a href="images/Awards/IranOpen.jpg">3rd place. Click to see the certificate</a></p>
	   				</li>
	   				</div>

	   			</div>
	   	     	<div class="timeline-block">


   					<div class="timeline-header">
	   					<h3>RoboCup Asia-Pacific</h3>
	   					<p>December 2018</p>
	   				</div>

	   				<div class="timeline-ico">
	   					<i class="fa fa-trophy"></i>
	   				</div>


	   				<div class="timeline-content">
	   					<h4>Unmanned Aerial Vehicle (UAV) league</h4>
	   					<li>
	   					<p><a href="images/Awards/AsiaPacific.jpg">6th place. Click to see the certificate</a></p>
	   				</li>
	   			</div>
	   		</div>

   		</div> <!-- /col-twelve -->
   		
   	</div> <!-- /resume-timeline -->
   	<div class="row resume-timeline">

   		<div class="col-twelve resume-header">

   			<h2>Projects</h2>

   		</div> <!-- /resume-header -->

   		<div class="col-twelve">

   			<div class="timeline-wrap">

   				<div class="timeline-block">

   					<div class="timeline-header">
	   					<h3>Rainbow</h3>
	   					<p>Jun 2020 – Jul 2020</p>
	   				</div>

	   				<div class="timeline-ico">
	   					<i class="fa fa-keyboard-o"></i>
	   				</div>


	   				<div class="timeline-content">
	   					<h4>Combining Improvements in Deep Reinforcement Learning. <a href="https://github.com/alirezakazemipour/Rainbow-Atari">[Project Page]</a></h4>
						<p>    When DQN was proposed although it was able to perform on Human-Level Performance on some Atari games like Pong, it suffered from different defects.
						Gradually various papers were introduced to undermine those problems of the DQN and Rainbow came last and showed each improvement can be combined with another one and combined all them together and showed that it has higher performance than each other combination of improvements.
						<!-- <br>Here is how each individual improvement was helpful: -->
						<ol><li>Double Q-Learning: Reduce overestimation.</li>
						<li>Dueling architecture: Reduce variance.</li>
						<li>Prioritized experience replay: Use rare/more-valuable experiences more frequently.</li>
						<li>Distributional reinforcement learning: Instead of the expected value of the discounted returns, learn the distribution that governs it</li>
						<li>Noisy Nets: Explore more intelligently than Epsilon-Greedy.</li>
						<li>Multi-Step Learning: Reduces bias in the short-term horizon and variance in the long-term horizon.</li>
						</ol>
					   </p>
	   				<img src="gifs/rainbow.gif"/>

	   				</div>

	   			</div>
	   	     	<div class="timeline-block">


   					<div class="timeline-header">
	   					<h3>Exploration by Random Network Distillation</h3>
	   					<p>Oct 2020 - Nov 2020</p>
	   				</div>

	   				<div class="timeline-ico">
	   					<i class="fa fa-keyboard-o"></i>
	   				</div>


	   				<div class="timeline-content">
	   					<h4>RND on Montezuma's Revenge and Super Mario Bros. <a href="https://github.com/alirezakazemipour/PPO-RND">[Project Page]</a></h4>
	   					<p>    Implementation of "Exploration by Random Network Distillation" to solve the Atari game Montezuma's Revenge which is an extremely sparse-reward environment.
	   					This algorithm also solved the first level of the Retro game Super Mario Bros.
						<br>   Primarily Count-Base exploration methods seemed to be a good representative of the novelty that the agent is facing but the Random Network Distillation showed that novelty can be expressed in a much straightforward manner as if no other methods (with even complicated approaches) can achieve its performance on the game of Montezuma's Revenge.</p>
   					<img src="gifs/RNN_Policy.gif"/>		
	   				</div>

	   	     	<div class="timeline-block">


   					<div class="timeline-header">
	   					<h3>Proximal Policy Optimization Algorithms</h3>
	   					<p>Feb 2020 - Sep 2020</p>
	   				</div>

	   				<div class="timeline-ico">
	   					<i class="fa fa-keyboard-o"></i>
	   				</div>


	   				<div class="timeline-content">
	   					<h4>Policy gradient methods that alternate between sampling data and optimizing a ”surrogate” objective function. 
	   						<br><a href="https://github.com/alirezakazemipour/Proximal-Policy-Optimization">[Atari Project Page]</a><a href="https://github.com/alirezakazemipour/Mujoco-PPO">[MuJoCo Project Page]</a><a href="https://github.com/alirezakazemipour/Mario-PPO">[Mario Project Page]</a></h4>
	   					<p>In terms of Sample Efficiency and Ease of implementation, Proximal Policy Optimization (PPO) is currently inimitable. It is an extension to Trust Region Policy Optimization (TRPO) that constrains the policy updates to be not far from the current policy.
						<br>PPO is applicable both in discrete and continuous spaces and uses General Advantage Estimation (GAE) to reduce variance common in Policy Gradient Methods.
						This project was carried on the Atari game Breakout, some MuJoCo environments such as Ant, and the game of Super Mario Bros.</p>
					</div>
	   			</div>

	   	     	<div class="timeline-block">


   					<div class="timeline-header">
	   					<h3>Soft Actor-Critic</h3>
	   					<p>Mar 2020 - Sep 2020</p>
	   				</div>

	   				<div class="timeline-ico">
	   					<i class="fa fa-keyboard-o"></i>
	   				</div>


	   				<div class="timeline-content">
	   					<h4>Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. 
	   					<a href="https://github.com/alirezakazemipour/SAC">[Project Page]</a></h4>
	   					<p>SAC, in general, seeks to maximize the entropy of the agent as well as its expected returns because having a suitable and maximized entropy has two crucial benefits:
	   						<ol>
						<li>It prevents the agent to be stuck in local minimums</li>
						<li>It helps the agent to explore widely</li>
					By using both the papers individually the hard continuous problem of Humanoid could be successfully solved.</p>
							</ol>
					</div>
	   			</div>

	   	     	<div class="timeline-block">


   					<div class="timeline-header">
	   					<h3>Discrete Soft Actor-Critic</h3>
	   					<p>Dec 2020</p>
	   				</div>

	   				<div class="timeline-ico">
	   					<i class="fa fa-keyboard-o"></i>
	   				</div>


	   				<div class="timeline-content">
	   					<h4>Soft Actor-Critic For Discrete Action Settings. 
	   					<a href="https://github.com/alirezakazemipour/Discrete-SAC-PyTorch">[Project Page]</a></h4>
	   					<p>While Soft Actor-Critic (SAC) is known to be the state-of-the-art model-free reinforcement algorithm for continuous action settings, it is not applicable to discrete action spaces (e.g Atari domain).
						<br>The most appealing property of SAC is that it tries to maximize Entropy as well as the original RL objective, Discounted Returns.
						The change that is needed to make SAC applicable to discrete spaces is that since the probability of each action in each state is known, all Expectations are transformed to the product of each probability.
						<br>This algorithm was tested on the Pacman Atari game.</p>
					</div>
	   			</div>

	   	     	<div class="timeline-block">


   					<div class="timeline-header">
	   					<h3>DDPG + HER</h3>
	   					<p>Feb 2020 - Sep 2020</p>
	   				</div>

	   				<div class="timeline-ico">
	   					<i class="fa fa-keyboard-o"></i>
	   				</div>


	   				<div class="timeline-content">
	   					<h4>Deep deterministic Policy Gradient and Hindsight Experience Replay. 
	   					<a href="https://github.com/alirezakazemipour/DDPG-her">[Project Page]</a></h4>
	   					<p>The idea behind Hindsight Experience (HER) is so simple but also, it is so elegant and powerful to solve Multi Sparse-Goal environments.
						<br>HER changes the problem into some sub-problems but with having an eye on what is desirable in the end. HER faces the problem in a format that if the agent fails to achieve the real and ultimate goal of the environment, he should be penalized but still, the agent should be appreciated since at least he has done sth and he has got even a very little but closer to the goal.
						<br>Thus, in HER setting there are two types of goals: 
						<ol>
							<li>The real goal of the problem</li>
							<li>The auxiliary goal that is given externally to the agent for encouraging him to learn to be active</li>
						</ol>
						This simple idea was combined with Deep Deterministic Policy Gradient method (which can be seen as an equivalent for DQN in continuous space) to solve Fetch, Pick and Place environment that requires a robot manipulator to find and place an object in various locations.</p>
					</div>
	   			</div>

	   	     	<div class="timeline-block">


   					<div class="timeline-header">
	   					<h3>Twin Delayed Deep Deterministic Policy Gradient</h3>
	   					<p>Jan 2021</p>
	   				</div>

	   				<div class="timeline-ico">
	   					<i class="fa fa-keyboard-o"></i>
	   				</div>


	   				<div class="timeline-content">
	   					<h4>Addressing Function Approximation Error in Actor-Critic Methods. 
	   					<a href="https://github.com/alirezakazemipour/TD3-PyTorch">[Project Page]</a></h4>
	   					<p>Overestimation has been proved to essentially exist in Temporal-Difference Learning (e.g., Q-Learning) by Neural Nets (like DQN) as a result of the "max" operator in Bellman Equation.
						<br>One established remedy for this defect is Double Q-Learning however, Double Q-Learning is only applicable in discrete-action settings.
						<br>An analogous method to DQN in continuous-action spaces is Deep Deterministic Policy Gradient (DDPG) although, DDPG is in Actor-Critic settings but TD3 paper showed that DDPG still suffers from Overestimation.
						<br>Thus, TD3 proposed a similar idea to Double Q-Learning for continuous-action spaces based on DDPG method and mitigated this undesirable effect considerably though, it could not completely diminish Overestimation.
						<br>Also, since DDPG is a Policy Gradient method it suffers from high variance hence, TD3 utilized tricks like Target Policy Smoothing Regularization to reduce the Variance.
						<br>An analogous method to DQN in continuous-action spaces is Deep Deterministic Policy Gradient (DDPG) although, DDPG is in Actor-Critic settings but TD3 paper showed that DDPG still suffers from Overestimation.
						<br>Thus, TD3 proposed a similar idea to Double Q-Learning for continuous-action spaces based on DDPG method and mitigated this undesirable effect considerably though, it could not completely diminish Overestimation.
						<br>Also, since DDPG is a Policy Gradient method it suffers from high variance hence, TD3 utilized from tricks like Target Policy Smoothing Regularization to reduce the Variance.</p>
					</div>
	   			</div>

	   	     	<div class="timeline-block">


   					<div class="timeline-header">
	   					<h3>CycleGAN</h3>
	   					<p>Mar 2020 - Nov 2020</p>
	   				</div>

	   				<div class="timeline-ico">
	   					<i class="fa fa-keyboard-o"></i>
	   				</div>


	   				<div class="timeline-content">
	   					<h4>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. 
	   					<a href="https://github.com/alirezakazemipour/Cycle-GAN-PyTorch">[Project Page]</a></h4>
	   					<p>CycleGan is proposed for the Image-to-Image translation problem when paired training data is not available.
						<br>One key consideration that CycleGan firmly prioritizes to develop its idea is that each mapping of X to Y (X -> Y) and Y to X images(Y -> X) should be reversible.
						<br>Also, CycleGan emphasizes that each mapping should hold Cycle Consistency condition to put constraint on the original problem which is:
						<ol>
							<li>X -> Y -> G(Y) such that: X = G(Y)</li>
							<li>Y -> X ->F(X) such that: Y = F(Y)</li>
						</ol>
					By using these ideas, two mappings of Horse-to-Zebra and Zebra-to-Horse was developed.</p>
					</div>
	   			</div>

	   		</div>

   		</div> <!-- /col-twelve -->
   		
   	</div> <!-- /resume-timeline -->
		
	</section>

	
	
   <!-- contact
   ================================================== -->
	<section id="contact">

		<div class="row section-intro">
   		<div class="col-twelve">

   			<h5>Contact</h5>

   	<div class="row contact-info">

   		<div class="col-four tab-full">

   			<div class="icon">
   				<i class="icon-pin"></i>
   			</div>

   			<h5>Where to find me</h5>

   			<p>
            Tehran, Iran
            </p>

   		</div>

   		<div class="col-four tab-full collapse">

   			<div class="icon">
   				<i class="icon-mail"></i>
   			</div>

   			<h5>Email Me At</h5>

   			<p>kazemipour.alireza@gmail.com			     
			   </p>

   		</div>

   		<div class="col-four tab-full">

   			<div class="icon">
   				<i class="icon-phone"></i>
   			</div>

   			<h5>Call Me At</h5>

   			<p>(+98)09128607650
			   </p>

   		</div>
   		
   	</div> <!-- /contact-info -->
		
	</section> <!-- /contact -->


   <!-- footer
   ================================================== -->

   <footer>

      	<div class="row">
	      	<div class="copyright">
		        	<span>© Copyright Kards 2016.</span> 
		        	<span>Design by <a href="http://www.styleshout.com/">styleshout</a></span>	         	
		         </div>		                  
	      	</div>

	      	<div id="go-top">
		         <a class="smoothscroll" title="Back to Top" href="#top"><i class="fa fa-long-arrow-up"></i></a>
		      </div>

      	</div> <!-- /row -->     	
   </footer>  

   <div id="preloader"> 
    	<div id="loader"></div>
   </div> 

   <!-- Java Script
   ================================================== --> 
   <script src="js/jquery-2.1.3.min.js"></script>
   <script src="js/plugins.js"></script>
   <script src="js/main.js"></script>

</body>

</html>
