---
title: "Some cool questions"  
permalink: /posts/2025/10/cl-qstns/ 
date: 2025-10-31  
tags:
- learning
---
 
[//]: Latex Macros  
  
$$  
\begin{align*}  
\newcommand{\I}{\mathbb{1}}  
\newcommand{\R}{\mathbb{R}}  
\newcommand{\Q}{\mathbb{Q}}  
\newcommand{\N}{\mathbb{N}}  
\DeclareMathOperator{\EE}{\mathbb{E}}  
\DeclareMathOperator{\PP}{\mathbb{P}}  
\newcommand{\Ev}[1]{\EE\left[ #1 \right]}  
\newcommand{\Pr}[1]{\PP\left( #1 \right)}  
\end{align*}  
$$
  - An elementary theorem in number theory states that if two integers $m$ and $n$ are relatively prime (i.e., greatest common divisor equal to 1 ), then there exist integers $x$ and $y$ (positive or negative) such that $mx + ny = 1$. Using this theorem show the following:            
  A If $m$ and $n$ are relatively prime then the set {$xm + ny : x, y$ positive integers} contains all but a finite number of the positive integers.            
  B. Let $J$ be a set of nonnegative integers whose greatest common divisor is $d$. Suppose also that $J$ is closed under addition, $m, n \in J \Rightarrow m + n \in J$. Then $J$ contains all but a finite number of integers in the set {$0, d, 2d,\dots$}.            
                
Source: [Introduction to Stochastic Processes](https://archive.org/details/introduction-to-stochastic-process-lawler/mode/1up?view=theater)            
            
_answer_.            
            
Part (A).            
            
First, note that if we want to contain only positive integers, then $n$ and $m$ must be nonnegative. If one of them is negative and the other is positive, we would inevitably contain the whole integers. Also, to rule out the trivial case, let us assume $n, m \geq 1$.            
            
Since $\mathsf{gcd} (n, m) = 1$, the great common divisor of any multiples of $n$ from $1n$ to $mn$ with $m$ is also one. In other words, the set {$n, 2n, \dots, mn$} forms the residue system modulo $m$. Let $N$ be any nonzero integer, we have $N \equiv ny \pmod m$, where $1 \leq y \leq m$. Equivalently, we can say $N - ny = mx$ for some integer $x$.            
            
We know that since $1 \leq y \leq m$, $y$ is positive. On the other hand, if $N \geq mn + 1$, then $x = \frac{N - ny}{m} > 0$, hence $x$ is also positive. Therefore, we just proved that for positive integers $x$ and $y$, all positive integers bigger than $mn$ are in the set described in the question.             
            
Part (B).            
            
Consider the set $J' =$ {$\frac{j}{d} \mid j \in J$}. If we show for any integer $k$ in $J'$ that is bigger than some threshold $K$, all but a finite number of nonnegative integers are in $J'$, then we can conclude for any integer $k$ in $J$ that is bigger than $Kd$, all but a finite number of nonnegative integers are in $J$            
            
By construction, the greatest common divisor of $J'$s elements is one and also $J'$ is closed under addition. Now, consider an arbitrary element in $J'$ and let us denote it by $a$. The set of remainder of $J'$s elements when divided by $a$ is $R = $ {$j' \pmod{a} \mid j' \in J'$}. Since the greatest common divisor of $J'$ is one, the greatest common divisor of $R$ is also one. Therefore, $R$ forms the residue system modulo $a$, i.e., $R = $ {$0, 1, \dots a - 1$}, where for each element of $r \in R$ there exits at least one element in $J'$ denoted by $j'_r$ such that $j'_r \equiv r \pmod{a}$. Let us denote the largest representative of these elements by $K$:            
            
$$K = \max \left\{j'_0, j'_1, \dots j'_{a - 1} \right\}.$$            
            
Now we show that any integer bigger than $K$ is in $J'$. Let $k$ be an integer bigger than $K$ and let its remainder when divided by $a$ be $r$, i.e., $k = r \pmod{ a}$. We know that there is an element in $J'$ with same remainder, i.e., $j'_r$, therefore:            
            
$$k = j'_r + c\cdot a,$$            
          
for some integer $c$. Since $k > M$ and $j'\_r \leq M$, $k - j'\_r > 0$ and $c \cdot a$ is positive multiple of $a$.            
Since $j'\_r, a \in J'$, and $J'$ is closed under addition, both $j'\_r$ and $c \cdot a = \underbrace{a + a + \dots + a}_{c\, \mathrm{times}}$ are in $J'$.            
Therefore, their sum, $k$, must also be in $J'$. This proves that $J'$ contains all integers greater than $K$.           
Consequently, $J$ contains {$(K + 1)d, (K + 2)d, \dots$}, which completes the proof.          
      
---  - What is an ergodic Markov chain?          
          
_answer_.          
          
It's an aperiodic, irreducible, and recurrent Markov chain:          
          
Aperiodic:          
The period of the chain is one.          
          
Irreducible:          
If there is only one communication class, i.e., if for all $i, j$ there exists an $n = n(i,j)$ with $P_n(i,j) > 0$, then the chain is called irreducible.          
          
Recurrent:          
If the chain starts in a transient class (set of states), then with probability one it eventually leaves this class and never returns. Classes with this property are called transient classes and the states are called transient states. Other classes are called recurrent classes with recurrent states. A Markov chain starting in a recurrent class never leaves that class.          
      
---  - Why is the left eigenvector of a transition matrix with the eigen value of one equal to the stationary distribution of the Markov chain modelled by the transition matrix?          
          
_answer_.           
          
Suppose $\bar{\pi}$ is the stationary distribution and $P$ is the transition matrix. Then, we know that since all rows of $P^\infty$ are the same, then for any initial any probability vector $\bar{v}$ we have          
          
$$\bar{\pi} = \lim_{n \to \infty} \bar{v}P^n.$$          
          
Hence, we have,          
          
$$\bar{\pi} = \lim_{n \to \infty} \bar{v}P^{n + 1} = \left(\lim_{n \to \infty} \bar{v}P^{n}\right)P = \bar{\pi}P.$$          
The above display is a left eigenvector of $P with eigenvalue one.          
        
Source: [Introduction to Stochastic Processes](https://archive.org/details/introduction-to-stochastic-process-lawler/mode/1up?view=theater)        
      
---  - Prove that every state in an irreducible Markov chain has the same period.         
        
The period of state $i$ is defined as the greatest common divisor of the set $J_i :=$ {$n \geq 1: P_n(i, i) >0$}. Not that $J_i$ is closed under addition because $P_{n +m} (i, i) = \sum_k P_n(i, k) P_m(k, i) \geq P_n(i, i) P_m(i, i) > 0$. Let $d$ be the greatest common divisor of $J_i$. We have shown before that $J_i$ contains all but a finite number of integers. Hence, $J_i$ contains $md$ for all $m$ greater than some integer $M$.        
        
Let $j$ be another state and $n,m$ such that $P_n(i, j), P_m(j, i) > 0$ (chain is irreducible). Clearly $n + m \in J_i$ and $m + n \in J_j$. If $l \in J_j$, then        
        
$$P_{n+m+l}(i, j) \geq P_{n}(i, j)P_l(j, j)P_m(j, i) > 0.$$        
        
Therefore, $n+m+l \in J_i, J_j$. $d$ used to divide $n + m$, now we showed that it must divide $l$ as well.         
So, we have just shown that if $d $divides every element of $J_i$        
then it divides every element of $J_j$. From this we see that all states have the same period.        
        
Source: [Introduction to Stochastic Processes](https://archive.org/details/introduction-to-stochastic-process-lawler/mode/1up?view=theater)      
      
---  - Consider the Markov chain with state space {1, 2, 3, 4, 5} and matrix      
      
$$ P = \begin{pmatrix}      
0 & 1/3 & 2/3 & 0 & 0 \\      
0 & 0 & 0 & 1/4 & 3/4 \\      
0 & 0 & 0 & 1/2 & 1/2 \\      
1 & 0 & 0 & 0 & 0 \\      
1 & 0 & 0 & 0 & 0      
\end{pmatrix}$$      
      
What are $P_{1000}(2, 1 ), P_{1000}(2, 2), P_{1000}(2, 4)$?       
      
Due to the structure of the chain at no time step there would be probability for transitioning from state 2 to state 1, nor from state 2 to itself. Hence, $P_{1000}(2, 1) = P_{1000}(2, 2) = 0$      
      
On the other hand to compute $P_{1000}(2, 4)$, since the chain is periodic with the period of three and the remainder of $1000$ when divided by $3$ is one, the same as number $4$, hence using Python software we raise the matrix to the power of $4$ instead, and $P_{1000}(2, 4) \approx P_4(2, 4) \approx  0.4167$.      
      
$$      
\begin{equation*}      
    P_4 = \begin{pmatrix}      
  0 & 0.3333 & 0.6665 & 0 & 0\\      
  0 & 0 & 0 & 0.4167 & 0.5835 \\      
  0 & 0 & 0 & 0.4167 & 0.5835\\      
  1 & 0 & 0 & 0 & 0 \\      
  1 & 0 & 0 & 0 & 0       
    \end{pmatrix}      
\end{equation*}      
$$      
      
Source: [Introduction to Stochastic Processes](https://archive.org/details/introduction-to-stochastic-process-lawler/mode/1up?view=theater)      
      
---  - Let $X_1, X_2,\dots$ be the successive values from independent rolls of a standard six-sided die. Let $S_n= X_1 + \dots + X_n$. Let      
      
$$      
T_1 = \min \{n \geq 1: S_n \, \text{is divisible by 8} \},      
$$      
      
$$      
T_2 = \min\{n \geq 1: S_{n - 1}\, \text{is divisible by 8}\}.      
$$      
      
Find $\mathbb{E}[T_1]$ and $\mathbb{E}[T_2]$. (Hint: consider the remainder of $S_n after division by 8 as a Markov chain.)      
      
_answer_.      
      
The state space of the chain comprises $\{0, 1, 2, 3, 4, 5, 6, 7\}$. Let $g(i)$ denotes the expected number of rolls until the chain reaches state zero, while starting at state $i$. We have that $g(0) = 0$. Definitely we won't reach the state $0$ with just one roll because the outcome would be among $1$ and $6$. Hence,      
      
$$      
\begin{equation*}      
    \mathbb{E}[T_1] = 1 + \frac{1}{6}\sum_{i = 1}^6 g(i) = 1 + \frac{1}{6}\left(g(1) + g(2) + g(3) + g(4) + g(5) + g(6)\right).      
\end{equation*}      
$$      
      
On the other hand,      
      
$$      
\begin{equation*}      
    \sum_{i=1}^7g(i) = \sum_{i=1}^7 \left(1 + \frac{1}{6}\sum_{j=1}^6 g\left(i + j \bmod{8}\right) \right).      
\end{equation*}      
$$      
      
That is, the expected number of rolls starting from state $i \neq 0$ is equal to rolling the die and the expected number of rolls of the resulting state. We rearrange the summations to get:      
      
$$      
\begin{equation*}      
    \sum_{i=1}^7g(i) =  7 + \frac{1}{6}\sum_{j=1}^6\sum_{i=7}^6 g(i + j \bmod{8}).      
\end{equation*}      
$$      
      
Now for each fixed $j$, $(i + j \bmod{8})$ is a permutation of $\{0, 1, 2, 3, 4, 5, 6, 7\}$ excluding the number $j$.       
Hence, by denoting $S := \sum_{i = 1}^7 g(i)$, we have:      
      
$$      
\begin{align*}      
    S = 7 + \frac{1}{6}\sum_{j=1}^6(S - g(j)) \Rightarrow \sum_{j = 1}^6 g(j) = 42.      
\end{align*}      
$$      
      
Therefore,      
      
$$      
\begin{equation*}      
    \mathbb{E}[T_1] = 1 + \frac{1}{6}\sum_{i = 1}^6 g(i) = 1 + \frac{1}{6}\left(g(1) + g(2) + g(3) + g(4) + g(5) + g(6)\right) = 8.      
\end{equation*}      
$$      
      
For computing $\mathbb{E}[T_2]$, we only need to roll the die once, because the sum of the random variables before the first roll is zero, so $S_{1 - 1} = S_0 = 0$, and $0$ is divisible by 8.       
      
Source: [Introduction to Stochastic Processes](https://archive.org/details/introduction-to-stochastic-process-lawler/mode/1up?view=theater)      
      
---  - Let $X_n, Y_n$ be independent Markov chains with state space ${0, 1, 2}$ and transition matrix      
      
$$      
P = \begin{pmatrix}      
1/2 & 1/4 & 1/4 \\      
1/4 & 1/4 & 1/2 \\      
0 & 1/2 & 1/2      
\end{pmatrix}.      
$$      
      
Suppose $X_0 = 0, Y_0 = 2$ and let      
      
$$T = \inf\{n: X_n = Y_n\}. $$      
      
(A) Find $\mathbb{E}[T]$.      
      
(B) What is $\mathbb{P}{X_T = 2}$?      
      
(C) In the long run, what percentage of the time are both chains in the same state?      
[Hint: consider the nine-state Markov chain $Z_n = (X_n, Y_n$).]      
      
_answer_.      
      
      
Part(A).      
      
We need to set up a recursive formula connecting the expected number of steps until two chains meet.       
      
Let $g(i, j)$ be the expected number of steps until the meeting time, when $\{X_n\}$ has started at state $i$ and $\{Y_n\}$ has started in state $j$. We have that $g(k, k) = 0,\: k \in \{0, 1, 2\}$. For all other state, since $\{X_n\}$ and $\{Y_n\}$ are independent, we have that:      
      
$$      
\begin{equation*}      
    g(i, j) = 1 + \sum_{k=0}^3\sum_{l=0}^3 P(i, k)P(j, l)g(k, j).      
\end{equation*}      
$$      
      
This gives a set of 6 equations with 6 unknowns       
($g(0, 1), g(0, 2), g(1, 0), g(1, 2), g(2, 0),\, \mathrm{and}\, g(2, 1)$) as follows:      
      
$$      
\begin{align*}      
    g(0, 1) & = 1 + \frac{1}{2}g(0, 0) + \frac{1}{4}g(0, 1) + \frac{1}{4}g(0, 2)  \\      
    g(0, 2) & = 1 + \frac{1}{2}g(0, 0) + \frac{1}{4}g(0, 1) + \frac{1}{4}g(0, 2)      
    \\      
    g(1, 0) & = 1 + \frac{1}{4}g(1, 0) + \frac{1}{4}g(1, 1) + \frac{1}{2}g(1, 2) \\      
    g(1, 2) & = 1 + \frac{1}{4}g(1, 0) + \frac{1}{4}g(1, 1) + \frac{1}{2}g(1, 2) \\      
    g(2, 0) & = 1 + 0\cdot g(2, 0) + \frac{1}{2}g(2, 1) + \frac{1}{2}g(2, 2) \\      
    g(2, 1) & = 1 + 0 \cdot g(2, 0) + \frac{1}{2}g(2, 1) + \frac{1}{2}g(2, 2).      
\end{align*}      
$$      
      
Solving this system of equations gives us: $g(0, 2) = \frac{118}{35} \approx 3.37$.      
      
Part (B).      
      
We need to set up a recursive formula connecting how the chains can meet at state 2 while starting at state 0 and 2 respectively.       
      
Let $h(i, j)$ be the probability of first meeting at state 2, when $\{X_n\}$ has started at state $i$ and $\{Y_n\}$ has started in state $j$. We have that $h(1, 1) = h(0 , 0) = 0$ (first meeting should be at state 2), and $h(2, 2) = 1$. For all other state, since $\{X_n\}$ and $\{Y_n\}$ are independent, we have that:      
      
$$      
\begin{equation*}      
    h(i, j) = \sum_{k=0}^3\sum_{l=0}^3 P(i, k)P(j, l)h(k, j).      
\end{equation*}      
$$      
      
This gives a set of 6 equations with 6 unknowns ($h(0, 1), h(0, 2), h(1, 0), h(1, 2), h(2, 0),\, \mathrm{and}\, h(2, 1)$) as follows:      
      
$$      
\begin{align*}      
    h(0, 1) & = \frac{1}{2}h(0, 0) + \frac{1}{4}h(0, 1) + \frac{1}{4}h(0, 2)  \\      
    h(0, 2) & = \frac{1}{2}h(0, 0) + \frac{1}{4}h(0, 1) + \frac{1}{4}h(0, 2)      
    \\      
    h(1, 0) & = \frac{1}{4}h(1, 0) + \frac{1}{4}h(1, 1) + \frac{1}{2}h(1, 2) \\      
    h(1, 2) & = \frac{1}{4}h(1, 0) + \frac{1}{4}h(1, 1) + \frac{1}{2}h(1, 2) \\      
    h(2, 0) & = 0\cdot h(2, 0) + \frac{1}{2}h(2, 1) + \frac{1}{2}h(2, 2) \\      
    h(2, 1) & = 0 \cdot h(2, 0) + \frac{1}{2}h(2, 1) + \frac{1}{2}h(2, 2).      
\end{align*}      
$$      
      
Solving this system of equations gives us: $h(0, 2) = \frac{15}{28} \approx 0.535$.      
      
Part (C).      
      
Since the chains are independent, we need to multiple their invariant distributions for states $\{1, 2, 3\}$. Let us find the invariant distribution by solving $\bar{\pi} = \bar{\pi}P$:      
      
$$      
\begin{equation*}      
    \begin{pmatrix}      
        \bar{\pi}_1 \\      
        \bar{\pi}_2 \\      
        \bar{\pi}_3      
    \end{pmatrix} =      
    \begin{pmatrix}      
        \bar{\pi}_1 \\      
        \bar{\pi}_2 \\      
        \bar{\pi}_3      
    \end{pmatrix}^\top \begin{pmatrix}      
        1/2 & 1/4 & 1/4  \\      
        1/4 & 1/4 & 1/2 \\      
        0 & 1/2 & 1/2      
    \end{pmatrix}      
    \quad \mathrm{and} \quad \bar{\pi}_1 + \bar{\pi}_2 + \bar{\pi}_3 = 1.      
\end{equation*}      
$$      
      
The solution of this equation is: $\bar{\pi} = \frac{1}{11}(2, 4, 5)$. Hence, the probability that both chains spend time at the same states in a long run is equal to: $\frac{1}{11^2}(4 + 16 + 25) = \frac{45}{121}$.      
      
Source: [Introduction to Stochastic Processes](https://archive.org/details/introduction-to-stochastic-process-lawler/mode/1up?view=theater)      
      
---  -  Let $X_n$ be a Markov chain on state space {1, 2, 3, 4, 5} with transition matrix      
      
$$      
P = \begin{pmatrix}      
0 & 1/2 & 1/2 & 0 & 0\\      
0 & 0 & 0 & 1/5 & 4/5 \\      
0 & 0 & 0 & 2/5 & 3/5 \\      
1 & 0 & 0 & 0 & 0 \\      
1/2 & 0 & 0 & 0 & 1/2      
\end{pmatrix}      
$$      
      
(A) Suppose $X_0 = 1$. What is the expected number of steps until the chain is in state 4?      
      
(B) Suppose $X_0 = 1$. What is the probability that the chain will enter state 5 before it enters state 3?      
      
_answer_.      
      
Part (A).      
      
We need to set up a recursive formula connecting the expected number of steps until reaching state 4 for each distinct state. Let $g(i)$ be the expected number of steps until reaching state $4$, when the chain has started at state $i$. We have that $g(4) = 0$. For all other state we have that:      
      
$$      
\begin{equation*}      
    g(i) = 1 + \sum_{j=0, j \neq 4}^5P(i, j)g(j).      
\end{equation*}      
$$      
      
This gives a set of 4 equations with 4 unknowns ($g(1), g(2), g(3),\, \mathrm{and}\, g(5)$) as follows:      
      
$$      
\begin{align*}      
    g(5) & = 1 + \frac{1}{2}g(0) + \frac{1}{2}g(5) \\      
    g(3) & = 1 + \frac{2}{5}g(4) + \frac{3}{5}g(5) \\      
    g(2) & = 1 + \frac{1}{5}g(4) + \frac{4}{5}g(5) \\      
    g(1) & = 1 + \frac{1}{2}g(2) + \frac{1}{2}g(3)      
\end{align*}      
$$      
      
Solving this system of equations gives us: $g(1) = \frac{34}{3}$.      
      
Part (B).      
      
We need to set up a recursive formula connecting how the chain can _hit_ state 5 before state 3, while starting at state 1.       
      
Let $h(i)$ be the probability that the chain hits state $5$ before state 3, when the chain has started at state $i$. We have that $h(5) =1$ and $h(3) = 0$. For all other state we have that:      
      
$$      
\begin{equation*}      
    h(i) = \sum_{j=1}^5P(i, j)h(j).      
\end{equation*}      
$$      
      
This gives a set of 3 equations with 3 unknowns ($h(1), h(2),\, \mathrm{and}\, h(4)$) as follows:      
      
$$      
\begin{align*}      
    h(4) & = h(1) \\      
    h(2) & = \frac{1}{5}h(4) + \frac{4}{5}h(5) =  \frac{1}{5}h(4) + \frac{4}{5}\\      
    h(1) & = \frac{1}{2}h(2) + \frac{1}{2}h(3) = \frac{1}{2}h(2)      
\end{align*}      
$$      
      
Solving this system of equations gives us: $h(1) = \frac{4}{9}$.      
      
Source: [Introduction to Stochastic Processes](https://archive.org/details/introduction-to-stochastic-process-lawler/mode/1up?view=theater)    
    
___  - __Theorem__. Suppose the state space $S$ and the action space $A$ are finite. Then, there exits a deterministic stationary Markov Blackwell optimal policy.    
    
_Proof_. Let $\Pi^\mathrm{MD}$ be the set of all deterministic Markov polices for an MDP with finite state and action spaces. Since $\Pi^\mathrm{MD}$ is finite, there exits a sequence of discount factors {$\gamma_n$} converging to one for which there exits a $\pi^* \in \Pi^\mathrm{MD}$ with $\left(\pi^*\right)^\infty = (\pi, \pi, \dots)  = (\pi_0, \pi_1, \dots)$ (as the stationary policy) being discount optimal for all $\gamma_n$.     
    
The reason the aforementioned fact is true is that since $\Pi^\mathrm{MD}$ finite and $0 \leq \gamma <1$ is infinite, due to pigeonhole principle some optimal policies are shared for some discount factors. Therefore, we can pick a subsequence of discount factors that increases toward one, which will have an associated optimal policy.    
    
With having the above fact in mind, for each $\pi \in \Pi^\mathrm{MD}$,    
    
$$v_\gamma^{\left(\pi^*\right)^\infty}(s) - v_\gamma^{\pi^\infty}(s) \geq 0,$$    
    
for all states, $\gamma = \gamma_n$ and $n = 1, 2, \dots$ . Each function on L.H.S. is a rational function of $\gamma$, so is their difference. Hence, the difference is zero for all $\gamma$, or equals zero for at most finitely many $\gamma$s. Therefore, there exists a $\gamma_\pi < 1$ for which the above display holds for $\gamma_\pi \leq \gamma < 1$. Since $\Pi^\mathrm{MD}$ is finite the above displays holds for all $\gamma^* \leq \gamma < 1$ where $\gamma^* = \max_\pi \gamma_{\pi}$.     
    
Now that we have fixed $\gamma$, by virtue of the existence of a deterministic stationary Markov policy in the discounted setting, the result follows. $\square$    
    
I've been concise and incomplete for this theorem. I'll add supplementary details one day that I stumbled on this post. :D    
    
Source: [Markov Decision Processes: Discrete Stochastic Dynamic Programming](https://personalpages.manchester.ac.uk/staff/mingfei.sun/books/mdp.pdf)  
  
___  
- Let $\Omega$ be a measurable set, and let $f: \Omega \to [0, \infty]$ be a non-negative measurable function. Prove that we have $0 \leq \int_\Omega f \leq \infty$. Furthermore, we have $\int_\Omega f = 0$ if and only if $f(x) = 0$ for almost every $x \in \Omega$.   
  
_Proof_. Since $f$ is a non-negative measurable function, we have that  
  
$$  
\int_\Omega f = \sup \{\int_\Omega s: s \text{ is non-negative, simple and dominated by } f \}.  
$$  
  
Step 1: $0 \leq \int_\Omega \leq \infty$.  
  
Consider a fixed $s$. Since $s$ is simple, $\int_\Omega s = \sum_{j = 1}^N c_j \cdot m(E_j)$,   
where $N \in \mathbb{N}, c_j > 0, m: \Omega \to \mathbb{R}^\*$ is the Lebesgue measure, $E\_1, \dots, E\_N \in  \Omega$,   
and $E_i \cap E_j = \varnothing$ for all $i, j \in [N], \mathrm{and}\, i \neq j$. The sum of non-negative terms on the  
extended real line $\mathbb{R}^*$ is in $[0, \infty]$, hence is their supremum.  
  
Step 2: If $f = 0$ a.e., then $\int_\Omega f = 0$.  
  
Since $f$ dominates $s$, then $0 \leq s(x) \leq f(x)$, for all $x \in \Omega$. If $f(x) = 0$ for almost every $x \in \Omega$, then $s = 0$ a.e., hence $\int_\Omega s = 0$, and consequently $\sup \int_\Omega s = 0$.  
  
Step 3: If $\int_\Omega f = 0$, then $f = 0$ a.e.  
  
Consider the set  
  
$$  
E_j := \{x \in \Omega: f(x) > \frac 1j \}, \quad j \geq 1.  
$$  
  
The function $s\_j := \frac 1j \mathbb{1}\_{E\_j}$ is simple and also dominated by $f$ because $0 \leq s_j \leq f$. By Step 1, we have $\frac 1j m(E_j) = \int_\Omega s_j \leq \int_\Omega f = 0$. This means $m(E_j) = 0$ for all $j$. The domain of $f$ where it is non zero is  
  
$$  
\Omega' := \{x : f(x) > 0 \} = \cup_{j = 1}^\infty E_j.  
$$  
  
By the subadditivity property of the Lebesgue measure we have   
  
$$  
m\left(\Omega' \right) = \sum_{j = 1}^\infty m(E_j) = 0 \Rightarrow f = 0\: \mathrm{a.e.}  
$$  
  
Source: [Analysis II](https://link.springer.com/book/10.1007/978-981-19-7284-3)

___
- Consider two independent Poisson processes $X(t)$ and $Y(t)$ with parameters $\lambda$ and $\mu$. Let $k \geq 1$ and define

$$
\begin{equation*}
\tau = \inf \{t > 0: X(t) =k \}, \quad \tau' = \inf \{t > 0: X(t) =k + 2\}.
\end{equation*}
$$

Find the distribution of $N = Y(\tau') - Y(\tau)$.

_Solution_. First we need to understand what the question actually asking us. $\tau' - \tau$
 is the time that it takes for {$X(t)$} to go from $k$ to $k+2$. We can write

$$
\begin{equation*}
U := T_{i + 2} - T_{i} = \underbrace{T_{i + 2} - T_{i + 1}}_{U_2} + \underbrace{T_{i + 1} - T_{i}}_{U_1},
\end{equation*}
$$

where $T_i = \inf \{t > 0: X(t) = i \}$. Each $U_i$ represents the arrival time of the $i+1$ "customer" after the fact that the $i$th customer has arrived. Hence, each $U_i$ follows the exponential distribtuion with parameter $\mu$. 

The question is asking us how much {$Y(t)$} will increase in the interval between the arrival time of the $k$th and $k+2$ customer at {$X(t)$}. So, we can write that 

$$
\begin{equation*}
\Pr{N = i \mid U = t} = \Pr{Y(U) = i \mid U = t} = \Pr{Y(t) = i} = e^{-\mu t} \cdot \frac{\left(\mu t\right)^i}{i!}.
\end{equation*}
$$

So,

$$
\begin{equation*}
\Pr{N = i} = \int_0^\infty \Pr{N = i \mid U = t} f_U(t) dt.
\end{equation*}
$$
So, we need the probability mass function of $U$. We said that $U = U_1 + U_2$ and each $U_i$ is comes from an independent exponential distribution. We have,

$$
\begin{align*}
f_U(t) &\stackrel{\text{(independence)}}{=} \int_0 ^ tf_{U_1}(x)\cdot f_{U_2}(t - x) dx \\
&= \int_0^t \lambda e^{-\lambda x} \cdot \lambda e^{-\lambda (t - x)} dx \\
& = \lambda^2 te^{-\lambda t}.
\end{align*}
$$

Therefore, 

$$
\begin{align*}
\Pr{N = i} & = \int_0^\infty \Pr{N = i \mid U = t} f_U(t) dt \\
& = \int_0^\infty e^{-\mu t} \frac{\left(\mu t\right)^i}{i!} \cdot \lambda^2 te^{-\lambda t} dt \\
& = \lambda^2\frac{\mu^i}{i!}\int_0^\infty t^{i + 1} \cdot e^{-(\lambda + \mu) t} dt \\
& = \lambda^2\frac{\mu^i}{i!}\int_0^\infty t^{i + 2 - 1} \cdot e^{-(\lambda + \mu) t} dt \\
& = \lambda^2\frac{\mu^i}{i!}\int_0^\infty t^{b - 1} \cdot e^{-a t} dt, \\
\end{align*}
$$

where $b = i + 2$ and $a = (\lambda + \mu)$. Let' see how to compute 

$$
\begin{equation}
\int_0^\infty t^{b - 1} \cdot e^{-a t} dt.
\end{equation}
$$

We know that for $z > 0$, the Gamma function is defined as

$$
\begin{equation*}
\Gamma(z) = (z - 1)! = \int_0^\infty t^{z - 1}e^{-t} dt.
\end{equation*}
$$

So with athe change of variables $u := at$ we have,

$$
\begin{equation}
\int_0^\infty t^{b - 1} \cdot e^{-a t} dt = \int_0^\infty \left(\frac{u}{a}\right)^{b - 1} \cdot e^{-u} \frac{du}{a} = \left(\frac{1}{a^{b + 1}}\right) \int_0^\infty u^{b - 1} \cdot e^{-u} du = \frac{\Gamma(b)}{a^{b + 1}}.
\end{equation}
$$

Now replacing $b = i + 2$ and $a = (\lambda + \mu)$,

$$
\begin{equation}
\int_0^\infty t^{b - 1} \cdot e^{-a t} dt = \frac{\Gamma(i + 2)}{(\lambda + \mu)^{i + 2}}.
\end{equation}
$$

So,

$$
\begin{align*}
\Pr{N = i} & = \lambda^2\frac{\mu^i}{i!}\int_0^\infty t^{b - 1} \cdot e^{-a t} dt, \\
& = \lambda^2\frac{\mu^i}{i!} \frac{\Gamma(i + 2)}{(\lambda + \mu)^{i + 2}} \\
& = \lambda^2\frac{\mu^i}{i!} \frac{(i + 1)!}{(\lambda + \mu)^{i + 2}} \\
& = (i + 1)\frac{\lambda^2\mu^i}{(\lambda + \mu)^{i + 2}} \\
& = \binom{i + 2 - 1}{i}\left(\frac{\mu}{\lambda + \mu}\right)^i \left( \frac{\lambda}{\lambda + \mu}\right)^2.
\end{align*}
$$

The negative binomial distribution with two success each with the probability of $\frac{\lambda}{ \lambda + \mu}$.

Source: [A question on my midterm at STAT 580 that I couldn't solve during the exam _:)_]

___
- Let $X_t$ be a Yule process with parameter $\lambda> 0$ and $X_0 =1$. In other words, $X_t$ is birth and death process with the birth rate $\lambda_n = n \lambda$ and the death rate $\mu_n = 0$. Show

$$
\begin{equation*}
P_n(t) = e^{-\lambda t}\left[1  - e^{-\lambda t}\right]^{n - 1}, \quad n\geq 1.
\end{equation*}
$$

Then, assume $X_t, Y_t$ are two independent Yule processes with parameter $\lambda$ and $X_0 = Y_0 = 1$. Determine the conditional distribution of $X_t$ given $X_t + Y_t = N,\; N \geq 2$.

_Solution._ In birth and death processes we have that

$$
\begin{align*}
\Pr{X_{t + \Delta t} = n \mid X_t = n} &= 1 - (\lambda_n + \mu_n) \Delta t + o(\Delta t) \\
\Pr{X_{t + \Delta t} = n+1 \mid X_t = n} &= \lambda_n \Delta t + o(\Delta t) \\
\Pr{X_{t + \Delta t} = n-1 \mid X_t = n} &= \mu_n \Delta t + o(\Delta t).
\end{align*}
$$

On the other hand, let $P_n(t) = \Pr{X_t =n}$. We have,

$$
\begin{align*}
P'_n(t) &= \lim_{\Delta t \to 0} \frac{P_n(t + \Delta t) - P_n(t)}{\Delta t} \\
&= \lim_{\Delta t \to 0} \frac{P_n(t)(1 - \lambda_n + \mu_n)\Delta t + \lambda_{n-1} P_{n - 1}(t) \Delta t + \mu_{n+1} P_{n + 1}(t) \Delta t - P_n(t)}{\Delta t} \\
& \stackrel{(\mu_n = 0)}{=}  \lambda_{n - 1} P_{n - 1}(t) - \lambda_n P_n(t) \\
& \stackrel{(\lambda_n = n\lambda)}{=} (n - 1)\lambda P_{n - 1}(t) - n\lambda P_n(t) \\ 
& \stackrel{\text{(replacing the solution)}}{=} \lambda e^{-\lambda t}\left[ 1 - e^{-\lambda t}\right]^{n - 1}\left(\frac{n - 1}{1 - e^{-\lambda t}}  - n\right) \\
& = \lambda e^{-\lambda t}\left[ 1 - e^{-\lambda t}\right]^{n - 1}\left(\frac{ ne^{-\lambda t} - 1}{1 - e^{-\lambda t}}\right).
\end{align*}
$$

Now we take the derivative of the proposed solution:

$$
\begin{align*}
-\lambda e^{-\lambda t}\left[1 - e^{-\lambda t}\right]^{n - 1} + (n - 1)\lambda\left[1 - e^{-\lambda t}\right]^{n - 2} e^{-2\lambda t} &= \lambda e^{-\lambda t}\left[ 1 - e^{-\lambda t}\right]^{n - 1} \left(-1 + \frac{e^{-\lambda t}(n - 1)}{1 - e^{-\lambda t}} \right) \\
& = \lambda e^{-\lambda t}\left[ 1 - e^{-\lambda t}\right]^{n - 1} \left(\frac{ne^{-\lambda t} - 1}{1 - e^{-\lambda t}} \right).
\end{align*}
$$

For the second part let $q = e^{-\lambda t}$. We have:

$$
\begin{align*}
\Pr{X_t = k \mid X_t + Y_t = N} &= \frac{\Pr{X_t = k, X_t + Y_t = N}}{\Pr{X_t + Y_t = N}} \\
&\stackrel{\text{(independence)}}{=} \frac{\Pr{X_t = k}\cdot \Pr{X_t + Y_t = N}}{\Pr{X_t + Y_t = N}} \\
&= \frac{\Pr{X_t = k}\cdot \Pr{Y_t = N - k}}{\sum_{i = 1}^{N - 1} \Pr{X_t = i} \cdot \Pr{Y_t = N - i}} \\
&\stackrel{\text{(part (a))}}{=} \frac{q(1 - q)^{k - 1} \cdot q(1 - q)^{N - k - 1}}{\sum_{i = 1}^{N - 1}q(1 - q)^{i - 1} \cdot q(1 - q)^{N - i - 1}} \\
&= \frac{(1 - q)^{N - 2}}{\sum_{i = 1}^{N - 1}(1 - q)^{N - 2}} = \frac{1}{N - 1}.
\end{align*}
$$

The uniform distribution.

Source: [A question on my midterm at STAT 580 that I couldn't solve during the exam _:)_]

___

