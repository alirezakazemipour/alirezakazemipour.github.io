

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Journal - Alireza Kazemipour</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Alireza Kazemipour">
<meta property="og:title" content="Journal">


  <link rel="canonical" href="https://alirezakazemipour.github.io/year-archive/">
  <meta property="og:url" content="https://alirezakazemipour.github.io/year-archive/">







  

  












  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Alireza Kazemipour",
      "url" : "https://alirezakazemipour.github.io",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="https://alirezakazemipour.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Alireza Kazemipour Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://alirezakazemipour.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://alirezakazemipour.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://alirezakazemipour.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://alirezakazemipour.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://alirezakazemipour.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://alirezakazemipour.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://alirezakazemipour.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://alirezakazemipour.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://alirezakazemipour.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://alirezakazemipour.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://alirezakazemipour.github.io/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://alirezakazemipour.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://alirezakazemipour.github.io/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://alirezakazemipour.github.io/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="https://alirezakazemipour.github.io/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://alirezakazemipour.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://alirezakazemipour.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://alirezakazemipour.github.io/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://alirezakazemipour.github.io/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://alirezakazemipour.github.io/">Alireza Kazemipour</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://alirezakazemipour.github.io/misc/">Miscellaneous</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://alirezakazemipour.github.io/unfin/">Unfinished ideas</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://alirezakazemipour.github.io/crs_prjcts/">Course projects</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://alirezakazemipour.github.io/year-archive/">Journal</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    



<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="https://alirezakazemipour.github.io/images/profile_pic.jpg" class="author__avatar" alt="Alireza Kazemipour">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Alireza Kazemipour</h3>
    <p class="author__bio">Student at the University of Alberta</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
      
      
      
      
       
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/alirezakazemipour"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.ca/citations?hl=en&view_op=list_works&gmla=AILGF5X_RbqMxSs8Nq6iXkVyL_lz-HNqZVZ_X2qSZugIJ8XuME1YDPSjjBkYnU3VwJ7q3MwRURMyDtY_6eDlOainzMGvPqujVrsR2cnNjzSM&user=L5AD6D0AAAAJ"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <div class="archive">
    
      <h1 class="page__title">Journal</h1>
    
    The life as a grad student taught me that I should maintain this page very seriously.




  
  
    <h2 id="2025" class="archive__subtitle">2025</h2>
    
  
  





<div class="list__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://alirezakazemipour.github.io/posts/2025/10/cl-qstns/" rel="permalink">Some cool questions
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock" aria-hidden="true"></i> 


  
	  27 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2025-10-31T00:00:00-07:00">October 31, 2025</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description">
<p>\(\begin{align*}  
\newcommand{\I}{\mathbb{1}}  
\newcommand{\R}{\mathbb{R}}  
\newcommand{\Q}{\mathbb{Q}}  
\newcommand{\N}{\mathbb{N}}  
\DeclareMathOperator{\EE}{\mathbb{E}}  
\DeclareMathOperator{\PP}{\mathbb{P}}  
\newcommand{\Ev}[1]{\EE\left[ #1 \right]}  
\newcommand{\Pr}[1]{\PP\left( #1 \right)}  
\end{align*}\)</p>
<ul>
  <li>An elementary theorem in number theory states that if two integers $m$ and $n$ are relatively prime (i.e., greatest common divisor equal to 1 ), then there exist integers $x$ and $y$ (positive or negative) such that $mx + ny = 1$. Using this theorem show the following:          <br />
  A If $m$ and $n$ are relatively prime then the set {$xm + ny : x, y$ positive integers} contains all but a finite number of the positive integers.          <br />
  B. Let $J$ be a set of nonnegative integers whose greatest common divisor is $d$. Suppose also that $J$ is closed under addition, $m, n \in J \Rightarrow m + n \in J$. Then $J$ contains all but a finite number of integers in the set {$0, d, 2d,\dots$}.</li>
</ul>

<p>Source: <a href="https://archive.org/details/introduction-to-stochastic-process-lawler/mode/1up?view=theater">Introduction to Stochastic Processes</a></p>

<p><em>answer</em>.</p>

<p>Part (A).</p>

<p>First, note that if we want to contain only positive integers, then $n$ and $m$ must be nonnegative. If one of them is negative and the other is positive, we would inevitably contain the whole integers. Also, to rule out the trivial case, let us assume $n, m \geq 1$.</p>

<p>Since $\mathsf{gcd} (n, m) = 1$, the great common divisor of any multiples of $n$ from $1n$ to $mn$ with $m$ is also one. In other words, the set {$n, 2n, \dots, mn$} forms the residue system modulo $m$. Let $N$ be any nonzero integer, we have $N \equiv ny \pmod m$, where $1 \leq y \leq m$. Equivalently, we can say $N - ny = mx$ for some integer $x$.</p>

<p>We know that since $1 \leq y \leq m$, $y$ is positive. On the other hand, if $N \geq mn + 1$, then $x = \frac{N - ny}{m} &gt; 0$, hence $x$ is also positive. Therefore, we just proved that for positive integers $x$ and $y$, all positive integers bigger than $mn$ are in the set described in the question.</p>

<p>Part (B).</p>

<p>Consider the set $J‚Äô =$ {$\frac{j}{d} \mid j \in J$}. If we show for any integer $k$ in $J‚Äô$ that is bigger than some threshold $K$, all but a finite number of nonnegative integers are in $J‚Äô$, then we can conclude for any integer $k$ in $J$ that is bigger than $Kd$, all but a finite number of nonnegative integers are in $J$</p>

<p>By construction, the greatest common divisor of $J‚Äô$s elements is one and also $J‚Äô$ is closed under addition. Now, consider an arbitrary element in $J‚Äô$ and let us denote it by $a$. The set of remainder of $J‚Äô$s elements when divided by $a$ is $R = $ {$j‚Äô \pmod{a} \mid j‚Äô \in J‚Äô$}. Since the greatest common divisor of $J‚Äô$ is one, the greatest common divisor of $R$ is also one. Therefore, $R$ forms the residue system modulo $a$, i.e., $R = $ {$0, 1, \dots a - 1$}, where for each element of $r \in R$ there exits at least one element in $J‚Äô$ denoted by $j‚Äô_r$ such that $j‚Äô_r \equiv r \pmod{a}$. Let us denote the largest representative of these elements by $K$:</p>

<p>[K = \max \left{j‚Äô<em>0, j‚Äô_1, \dots j‚Äô</em>{a - 1} \right}.]</p>

<p>Now we show that any integer bigger than $K$ is in $J‚Äô$. Let $k$ be an integer bigger than $K$ and let its remainder when divided by $a$ be $r$, i.e., $k = r \pmod{ a}$. We know that there is an element in $J‚Äô$ with same remainder, i.e., $j‚Äô_r$, therefore:</p>

<p>[k = j‚Äô_r + c\cdot a,]</p>

<p>for some integer $c$. Since $k &gt; M$ and $j‚Äô_r \leq M$, $k - j‚Äô_r &gt; 0$ and $c \cdot a$ is positive multiple of $a$.          <br />
Since $j‚Äô_r, a \in J‚Äô$, and $J‚Äô$ is closed under addition, both $j‚Äô_r$ and $c \cdot a = \underbrace{a + a + \dots + a}_{c\, \mathrm{times}}$ are in $J‚Äô$.          <br />
Therefore, their sum, $k$, must also be in $J‚Äô$. This proves that $J‚Äô$ contains all integers greater than $K$.         <br />
Consequently, $J$ contains {$(K + 1)d, (K + 2)d, \dots$}, which completes the proof.</p>

<hr />

<ul>
  <li>What is an ergodic Markov chain?</li>
</ul>

<p><em>answer</em>.</p>

<p>It‚Äôs an aperiodic, irreducible, and recurrent Markov chain:</p>

<p>Aperiodic:        <br />
The period of the chain is one.</p>

<p>Irreducible:        <br />
If there is only one communication class, i.e., if for all $i, j$ there exists an $n = n(i,j)$ with $P_n(i,j) &gt; 0$, then the chain is called irreducible.</p>

<p>Recurrent:        <br />
If the chain starts in a transient class (set of states), then with probability one it eventually leaves this class and never returns. Classes with this property are called transient classes and the states are called transient states. Other classes are called recurrent classes with recurrent states. A Markov chain starting in a recurrent class never leaves that class.</p>

<hr />

<ul>
  <li>Why is the left eigenvector of a transition matrix with the eigen value of one equal to the stationary distribution of the Markov chain modelled by the transition matrix?</li>
</ul>

<p><em>answer</em>.</p>

<p>Suppose $\bar{\pi}$ is the stationary distribution and $P$ is the transition matrix. Then, we know that since all rows of $P^\infty$ are the same, then for any initial any probability vector $\bar{v}$ we have</p>

<p>[\bar{\pi} = \lim_{n \to \infty} \bar{v}P^n.]</p>

<p>Hence, we have,</p>

<p>\(\bar{\pi} = \lim_{n \to \infty} \bar{v}P^{n + 1} = \left(\lim_{n \to \infty} \bar{v}P^{n}\right)P = \bar{\pi}P.\)        <br />
The above display is a left eigenvector of $P with eigenvalue one.</p>

<p>Source: <a href="https://archive.org/details/introduction-to-stochastic-process-lawler/mode/1up?view=theater">Introduction to Stochastic Processes</a></p>

<hr />

<ul>
  <li>Prove that every state in an irreducible Markov chain has the same period.</li>
</ul>

<p>The period of state $i$ is defined as the greatest common divisor of the set $J_i :=$ {$n \geq 1: P_n(i, i) &gt;0$}. Not that $J_i$ is closed under addition because $P_{n +m} (i, i) = \sum_k P_n(i, k) P_m(k, i) \geq P_n(i, i) P_m(i, i) &gt; 0$. Let $d$ be the greatest common divisor of $J_i$. We have shown before that $J_i$ contains all but a finite number of integers. Hence, $J_i$ contains $md$ for all $m$ greater than some integer $M$.</p>

<p>Let $j$ be another state and $n,m$ such that $P_n(i, j), P_m(j, i) &gt; 0$ (chain is irreducible). Clearly $n + m \in J_i$ and $m + n \in J_j$. If $l \in J_j$, then</p>

<p>[P_{n+m+l}(i, j) \geq P_{n}(i, j)P_l(j, j)P_m(j, i) &gt; 0.]</p>

<p>Therefore, $n+m+l \in J_i, J_j$. $d$ used to divide $n + m$, now we showed that it must divide $l$ as well.       <br />
So, we have just shown that if $d $divides every element of $J_i$      <br />
then it divides every element of $J_j$. From this we see that all states have the same period.</p>

<p>Source: <a href="https://archive.org/details/introduction-to-stochastic-process-lawler/mode/1up?view=theater">Introduction to Stochastic Processes</a></p>

<p>‚Äî  - Consider the Markov chain with state space {1, 2, 3, 4, 5} and matrix</p>

<p>[P = \begin{pmatrix}    <br />
0 &amp; 1/3 &amp; 2/3 &amp; 0 &amp; 0 \    <br />
0 &amp; 0 &amp; 0 &amp; 1/4 &amp; 3/4 \    <br />
0 &amp; 0 &amp; 0 &amp; 1/2 &amp; 1/2 \    <br />
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \    <br />
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0    <br />
\end{pmatrix}]</p>

<p>What are $P_{1000}(2, 1 ), P_{1000}(2, 2), P_{1000}(2, 4)$?</p>

<p>Due to the structure of the chain at no time step there would be probability for transitioning from state 2 to state 1, nor from state 2 to itself. Hence, $P_{1000}(2, 1) = P_{1000}(2, 2) = 0$</p>

<p>On the other hand to compute $P_{1000}(2, 4)$, since the chain is periodic with the period of three and the remainder of $1000$ when divided by $3$ is one, the same as number $4$, hence using Python software we raise the matrix to the power of $4$ instead, and $P_{1000}(2, 4) \approx P_4(2, 4) \approx  0.4167$.</p>

<p>[\begin{equation<em>}    <br />
    P_4 = \begin{pmatrix}    <br />
  0 &amp; 0.3333 &amp; 0.6665 &amp; 0 &amp; 0\    <br />
  0 &amp; 0 &amp; 0 &amp; 0.4167 &amp; 0.5835 \    <br />
  0 &amp; 0 &amp; 0 &amp; 0.4167 &amp; 0.5835\    <br />
  1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \    <br />
  1 &amp; 0 &amp; 0 &amp; 0 &amp; 0     <br />
    \end{pmatrix}    <br />
\end{equation</em>}]</p>

<p>Source: <a href="https://archive.org/details/introduction-to-stochastic-process-lawler/mode/1up?view=theater">Introduction to Stochastic Processes</a></p>

<hr />

<ul>
  <li>Let $X_1, X_2,\dots$ be the successive values from independent rolls of a standard six-sided die. Let $S_n= X_1 + \dots + X_n$. Let</li>
</ul>

<p>[T_1 = \min {n \geq 1: S_n \, \text{is divisible by 8} },]</p>

<p>[T_2 = \min{n \geq 1: S_{n - 1}\, \text{is divisible by 8}}.]</p>

<p>Find $\mathbb{E}[T_1]$ and $\mathbb{E}[T_2]$. (Hint: consider the remainder of $S_n after division by 8 as a Markov chain.)</p>

<p><em>answer</em>.</p>

<p>The state space of the chain comprises ${0, 1, 2, 3, 4, 5, 6, 7}$. Let $g(i)$ denotes the expected number of rolls until the chain reaches state zero, while starting at state $i$. We have that $g(0) = 0$. Definitely we won‚Äôt reach the state $0$ with just one roll because the outcome would be among $1$ and $6$. Hence,</p>

<p>[\begin{equation<em>}    <br />
    \mathbb{E}[T_1] = 1 + \frac{1}{6}\sum_{i = 1}^6 g(i) = 1 + \frac{1}{6}\left(g(1) + g(2) + g(3) + g(4) + g(5) + g(6)\right).    <br />
\end{equation</em>}]</p>

<p>On the other hand,</p>

<p>[\begin{equation<em>}    <br />
    \sum_{i=1}^7g(i) = \sum_{i=1}^7 \left(1 + \frac{1}{6}\sum_{j=1}^6 g\left(i + j \bmod{8}\right) \right).    <br />
\end{equation</em>}]</p>

<p>That is, the expected number of rolls starting from state $i \neq 0$ is equal to rolling the die and the expected number of rolls of the resulting state. We rearrange the summations to get:</p>

<p>[\begin{equation<em>}    <br />
    \sum_{i=1}^7g(i) =  7 + \frac{1}{6}\sum_{j=1}^6\sum_{i=7}^6 g(i + j \bmod{8}).    <br />
\end{equation</em>}]</p>

<p>Now for each fixed $j$, $(i + j \bmod{8})$ is a permutation of ${0, 1, 2, 3, 4, 5, 6, 7}$ excluding the number $j$.     <br />
Hence, by denoting $S := \sum_{i = 1}^7 g(i)$, we have:</p>

<p>[\begin{align<em>}    <br />
    S = 7 + \frac{1}{6}\sum_{j=1}^6(S - g(j)) \Rightarrow \sum_{j = 1}^6 g(j) = 42.    <br />
\end{align</em>}]</p>

<p>Therefore,</p>

<p>[\begin{equation<em>}    <br />
    \mathbb{E}[T_1] = 1 + \frac{1}{6}\sum_{i = 1}^6 g(i) = 1 + \frac{1}{6}\left(g(1) + g(2) + g(3) + g(4) + g(5) + g(6)\right) = 8.    <br />
\end{equation</em>}]</p>

<p>For computing $\mathbb{E}[T_2]$, we only need to roll the die once, because the sum of the random variables before the first roll is zero, so $S_{1 - 1} = S_0 = 0$, and $0$ is divisible by 8.</p>

<p>Source: <a href="https://archive.org/details/introduction-to-stochastic-process-lawler/mode/1up?view=theater">Introduction to Stochastic Processes</a></p>

<hr />

<ul>
  <li>Let $X_n, Y_n$ be independent Markov chains with state space {$0, 1, 2$} and transition matrix</li>
</ul>

<p>[P = \begin{pmatrix}    <br />
1/2 &amp; 1/4 &amp; 1/4 \    <br />
1/4 &amp; 1/4 &amp; 1/2 \    <br />
0 &amp; 1/2 &amp; 1/2    <br />
\end{pmatrix}.]</p>

<p>Suppose $X_0 = 0, Y_0 = 2$ and let</p>

<p>[T = \inf{n: X_n = Y_n}.]</p>

<p>(A) Find $\mathbb{E}[T]$.</p>

<p>(B) What is $\mathbb{P}{X_T = 2}$?</p>

<p>(C) In the long run, what percentage of the time are both chains in the same state?    <br />
[Hint: consider the nine-state Markov chain $Z_n = (X_n, Y_n$).]</p>

<p><em>answer</em>.</p>

<p>Part(A).</p>

<p>We need to set up a recursive formula connecting the expected number of steps until two chains meet.</p>

<p>Let $g(i, j)$ be the expected number of steps until the meeting time, when ${X_n}$ has started at state $i$ and ${Y_n}$ has started in state $j$. We have that $g(k, k) = 0,: k \in {0, 1, 2}$. For all other state, since ${X_n}$ and ${Y_n}$ are independent, we have that:</p>

<p>[\begin{equation<em>}    <br />
    g(i, j) = 1 + \sum_{k=0}^3\sum_{l=0}^3 P(i, k)P(j, l)g(k, j).    <br />
\end{equation</em>}]</p>

<p>This gives a set of 6 equations with 6 unknowns     <br />
($g(0, 1), g(0, 2), g(1, 0), g(1, 2), g(2, 0),\, \mathrm{and}\, g(2, 1)$) as follows:</p>

<p>[\begin{align<em>}    <br />
    g(0, 1) &amp; = 1 + \frac{1}{2}g(0, 0) + \frac{1}{4}g(0, 1) + \frac{1}{4}g(0, 2)  \    <br />
    g(0, 2) &amp; = 1 + \frac{1}{2}g(0, 0) + \frac{1}{4}g(0, 1) + \frac{1}{4}g(0, 2)    <br />
    \    <br />
    g(1, 0) &amp; = 1 + \frac{1}{4}g(1, 0) + \frac{1}{4}g(1, 1) + \frac{1}{2}g(1, 2) \    <br />
    g(1, 2) &amp; = 1 + \frac{1}{4}g(1, 0) + \frac{1}{4}g(1, 1) + \frac{1}{2}g(1, 2) \    <br />
    g(2, 0) &amp; = 1 + 0\cdot g(2, 0) + \frac{1}{2}g(2, 1) + \frac{1}{2}g(2, 2) \    <br />
    g(2, 1) &amp; = 1 + 0 \cdot g(2, 0) + \frac{1}{2}g(2, 1) + \frac{1}{2}g(2, 2).    <br />
\end{align</em>}]</p>

<p>Solving this system of equations gives us: $g(0, 2) = \frac{118}{35} \approx 3.37$.</p>

<p>Part (B).</p>

<p>We need to set up a recursive formula connecting how the chains can meet at state 2 while starting at state 0 and 2 respectively.</p>

<p>Let $h(i, j)$ be the probability of first meeting at state 2, when ${X_n}$ has started at state $i$ and ${Y_n}$ has started in state $j$. We have that $h(1, 1) = h(0 , 0) = 0$ (first meeting should be at state 2), and $h(2, 2) = 1$. For all other state, since ${X_n}$ and ${Y_n}$ are independent, we have that:</p>

<p>[\begin{equation<em>}    <br />
    h(i, j) = \sum_{k=0}^3\sum_{l=0}^3 P(i, k)P(j, l)h(k, j).    <br />
\end{equation</em>}]</p>

<p>This gives a set of 6 equations with 6 unknowns ($h(0, 1), h(0, 2), h(1, 0), h(1, 2), h(2, 0),\, \mathrm{and}\, h(2, 1)$) as follows:</p>

<p>[\begin{align<em>}    <br />
    h(0, 1) &amp; = \frac{1}{2}h(0, 0) + \frac{1}{4}h(0, 1) + \frac{1}{4}h(0, 2)  \    <br />
    h(0, 2) &amp; = \frac{1}{2}h(0, 0) + \frac{1}{4}h(0, 1) + \frac{1}{4}h(0, 2)    <br />
    \    <br />
    h(1, 0) &amp; = \frac{1}{4}h(1, 0) + \frac{1}{4}h(1, 1) + \frac{1}{2}h(1, 2) \    <br />
    h(1, 2) &amp; = \frac{1}{4}h(1, 0) + \frac{1}{4}h(1, 1) + \frac{1}{2}h(1, 2) \    <br />
    h(2, 0) &amp; = 0\cdot h(2, 0) + \frac{1}{2}h(2, 1) + \frac{1}{2}h(2, 2) \    <br />
    h(2, 1) &amp; = 0 \cdot h(2, 0) + \frac{1}{2}h(2, 1) + \frac{1}{2}h(2, 2).    <br />
\end{align</em>}]</p>

<p>Solving this system of equations gives us: $h(0, 2) = \frac{15}{28} \approx 0.535$.</p>

<p>Part (C).</p>

<p>Since the chains are independent, we need to multiple their invariant distributions for states ${1, 2, 3}$. Let us find the invariant distribution by solving $\bar{\pi} = \bar{\pi}P$:</p>

<p>[\begin{equation<em>}    <br />
    \begin{pmatrix}    <br />
        \bar{\pi}_1 \    <br />
        \bar{\pi}_2 \    <br />
        \bar{\pi}_3    <br />
    \end{pmatrix} =    <br />
    \begin{pmatrix}    <br />
        \bar{\pi}_1 \    <br />
        \bar{\pi}_2 \    <br />
        \bar{\pi}_3    <br />
    \end{pmatrix}^\top \begin{pmatrix}    <br />
        1/2 &amp; 1/4 &amp; 1/4  \    <br />
        1/4 &amp; 1/4 &amp; 1/2 \    <br />
        0 &amp; 1/2 &amp; 1/2    <br />
    \end{pmatrix}    <br />
    \quad \mathrm{and} \quad \bar{\pi}_1 + \bar{\pi}_2 + \bar{\pi}_3 = 1.    <br />
\end{equation</em>}]</p>

<p>The solution of this equation is: $\bar{\pi} = \frac{1}{11}(2, 4, 5)$. Hence, the probability that both chains spend time at the same states in a long run is equal to: $\frac{1}{11^2}(4 + 16 + 25) = \frac{45}{121}$.</p>

<p>Source: <a href="https://archive.org/details/introduction-to-stochastic-process-lawler/mode/1up?view=theater">Introduction to Stochastic Processes</a></p>

<hr />

<ul>
  <li>Let $X_n$ be a Markov chain on state space {1, 2, 3, 4, 5} with transition matrix</li>
</ul>

<p>[P = \begin{pmatrix}    <br />
0 &amp; 1/2 &amp; 1/2 &amp; 0 &amp; 0\    <br />
0 &amp; 0 &amp; 0 &amp; 1/5 &amp; 4/5 \    <br />
0 &amp; 0 &amp; 0 &amp; 2/5 &amp; 3/5 \    <br />
1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \    <br />
1/2 &amp; 0 &amp; 0 &amp; 0 &amp; 1/2    <br />
\end{pmatrix}]</p>

<p>(A) Suppose $X_0 = 1$. What is the expected number of steps until the chain is in state 4?</p>

<p>(B) Suppose $X_0 = 1$. What is the probability that the chain will enter state 5 before it enters state 3?</p>

<p><em>answer</em>.</p>

<p>Part (A).</p>

<p>We need to set up a recursive formula connecting the expected number of steps until reaching state 4 for each distinct state. Let $g(i)$ be the expected number of steps until reaching state $4$, when the chain has started at state $i$. We have that $g(4) = 0$. For all other state we have that:</p>

<p>[\begin{equation<em>}    <br />
    g(i) = 1 + \sum_{j=0, j \neq 4}^5P(i, j)g(j).    <br />
\end{equation</em>}]</p>

<p>This gives a set of 4 equations with 4 unknowns ($g(1), g(2), g(3),\, \mathrm{and}\, g(5)$) as follows:</p>

<p>[\begin{align<em>}    <br />
    g(5) &amp; = 1 + \frac{1}{2}g(0) + \frac{1}{2}g(5) \    <br />
    g(3) &amp; = 1 + \frac{2}{5}g(4) + \frac{3}{5}g(5) \    <br />
    g(2) &amp; = 1 + \frac{1}{5}g(4) + \frac{4}{5}g(5) \    <br />
    g(1) &amp; = 1 + \frac{1}{2}g(2) + \frac{1}{2}g(3)    <br />
\end{align</em>}]</p>

<p>Solving this system of equations gives us: $g(1) = \frac{34}{3}$.</p>

<p>Part (B).</p>

<p>We need to set up a recursive formula connecting how the chain can <em>hit</em> state 5 before state 3, while starting at state 1.</p>

<p>Let $h(i)$ be the probability that the chain hits state $5$ before state 3, when the chain has started at state $i$. We have that $h(5) =1$ and $h(3) = 0$. For all other state we have that:</p>

<p>[\begin{equation<em>}    <br />
    h(i) = \sum_{j=1}^5P(i, j)h(j).    <br />
\end{equation</em>}]</p>

<p>This gives a set of 3 equations with 3 unknowns ($h(1), h(2),\, \mathrm{and}\, h(4)$) as follows:</p>

<p>[\begin{align<em>}    <br />
    h(4) &amp; = h(1) \    <br />
    h(2) &amp; = \frac{1}{5}h(4) + \frac{4}{5}h(5) =  \frac{1}{5}h(4) + \frac{4}{5}\    <br />
    h(1) &amp; = \frac{1}{2}h(2) + \frac{1}{2}h(3) = \frac{1}{2}h(2)    <br />
\end{align</em>}]</p>

<p>Solving this system of equations gives us: $h(1) = \frac{4}{9}$.</p>

<p>Source: <a href="https://archive.org/details/introduction-to-stochastic-process-lawler/mode/1up?view=theater">Introduction to Stochastic Processes</a></p>

<hr />

<ul>
  <li><strong>Theorem</strong>. Suppose the state space $S$ and the action space $A$ are finite. Then, there exits a deterministic stationary Markov Blackwell optimal policy.</li>
</ul>

<p><em>Proof</em>. Let $\Pi^\mathrm{MD}$ be the set of all deterministic Markov polices for an MDP with finite state and action spaces. Since $\Pi^\mathrm{MD}$ is finite, there exits a sequence of discount factors {$\gamma_n$} converging to one for which there exits a $\pi^* \in \Pi^\mathrm{MD}$ with $\left(\pi^*\right)^\infty = (\pi, \pi, \dots)  = (\pi_0, \pi_1, \dots)$ (as the stationary policy) being discount optimal for all $\gamma_n$.</p>

<p>The reason the aforementioned fact is true is that since $\Pi^\mathrm{MD}$ finite and $0 \leq \gamma &lt;1$ is infinite, due to pigeonhole principle some optimal policies are shared for some discount factors. Therefore, we can pick a subsequence of discount factors that increases toward one, which will have an associated optimal policy.</p>

<p>With having the above fact in mind, for each $\pi \in \Pi^\mathrm{MD}$,</p>

<p>[v_\gamma^{\left(\pi^*\right)^\infty}(s) - v_\gamma^{\pi^\infty}(s) \geq 0,]</p>

<p>for all states, $\gamma = \gamma_n$ and $n = 1, 2, \dots$ . Each function on L.H.S. is a rational function of $\gamma$, so is their difference. Hence, the difference is zero for all $\gamma$, or equals zero for at most finitely many $\gamma$s. Therefore, there exists a $\gamma_\pi &lt; 1$ for which the above display holds for $\gamma_\pi \leq \gamma &lt; 1$. Since $\Pi^\mathrm{MD}$ is finite the above displays holds for all $\gamma^* \leq \gamma &lt; 1$ where $\gamma^* = \max_\pi \gamma_{\pi}$.</p>

<p>Now that we have fixed $\gamma$, by virtue of the existence of a deterministic stationary Markov policy in the discounted setting, the result follows. $\square$</p>

<p>I‚Äôve been concise and incomplete for this theorem. I‚Äôll add supplementary details one day that I stumbled on this post. :D</p>

<p>Source: <a href="https://personalpages.manchester.ac.uk/staff/mingfei.sun/books/mdp.pdf">Markov Decision Processes: Discrete Stochastic Dynamic Programming</a></p>

<hr />

<ul>
  <li>Let $\Omega$ be a measurable set, and let $f: \Omega \to [0, \infty]$ be a non-negative measurable function. Prove that we have $0 \leq \int_\Omega f \leq \infty$. Furthermore, we have $\int_\Omega f = 0$ if and only if $f(x) = 0$ for almost every $x \in \Omega$.</li>
</ul>

<p><em>Proof</em>. Since $f$ is a non-negative measurable function, we have that</p>

<p>[\int_\Omega f = \sup {\int_\Omega s: s \text{ is non-negative, simple and dominated by } f }.]</p>

<p>Step 1: $0 \leq \int_\Omega \leq \infty$.</p>

<p>Consider a fixed $s$. Since $s$ is simple, $\int_\Omega s = \sum_{j = 1}^N c_j \cdot m(E_j)$, <br />
where $N \in \mathbb{N}, c_j &gt; 0, m: \Omega \to \mathbb{R}^*$ is the Lebesgue measure, $E_1, \dots, E_N \in  \Omega$, <br />
and $E_i \cap E_j = \varnothing$ for all $i, j \in [N], \mathrm{and}\, i \neq j$. The sum of non-negative terms on the<br />
extended real line $\mathbb{R}^*$ is in $[0, \infty]$, hence is their supremum.</p>

<p>Step 2: If $f = 0$ a.e., then $\int_\Omega f = 0$.</p>

<p>Since $f$ dominates $s$, then $0 \leq s(x) \leq f(x)$, for all $x \in \Omega$. If $f(x) = 0$ for almost every $x \in \Omega$, then $s = 0$ a.e., hence $\int_\Omega s = 0$, and consequently $\sup \int_\Omega s = 0$.</p>

<p>Step 3: If $\int_\Omega f = 0$, then $f = 0$ a.e.</p>

<p>Consider the set</p>

<p>[E_j := {x \in \Omega: f(x) &gt; \frac 1j }, \quad j \geq 1.]</p>

<p>The function $s_j := \frac 1j \mathbb{1}_{E_j}$ is simple and also dominated by $f$ because $0 \leq s_j \leq f$. By Step 1, we have $\frac 1j m(E_j) = \int_\Omega s_j \leq \int_\Omega f = 0$. This means $m(E_j) = 0$ for all $j$. The domain of $f$ where it is non zero is</p>

<p>[\Omega‚Äô := {x : f(x) &gt; 0 } = \cup_{j = 1}^\infty E_j.]</p>

<p>By the subadditivity property of the Lebesgue measure we have</p>

<p>[m\left(\Omega‚Äô \right) = \sum_{j = 1}^\infty m(E_j) = 0 \Rightarrow f = 0: \mathrm{a.e.}]</p>

<p>Source: <a href="https://link.springer.com/book/10.1007/978-981-19-7284-3">Analysis II</a></p>

</p>
    

    

  </article>
</div>

  
  
  





<div class="list__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://alirezakazemipour.github.io/posts/2025/10/cnvrgnc-sprmrtngl/" rel="permalink">Convergence of Positive Supermartingales
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock" aria-hidden="true"></i> 


  
	  26 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2025-10-27T00:00:00-07:00">October 27, 2025</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description">
</p>
    

    

  </article>
</div>

  
  
  





<div class="list__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://alirezakazemipour.github.io/posts/2025/09/mrkv-chns/" rel="permalink">Some cool things about finite Markov chains and thereof
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock" aria-hidden="true"></i> 


  
	  12 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2025-09-23T00:00:00-07:00">September 23, 2025</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>In this post, I wanna summarize Sections 1.5 and 1.6 of <a href="https://archive.org/details/introduction-to-stochastic-process-lawler/mode/1up?view=theater">Introduction to Stochastic Processes</a> and kinda blew my mind.</p>

</p>
    

    

  </article>
</div>

  
  
  





<div class="list__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://alirezakazemipour.github.io/posts/2025/08/t-dstrbtn-p-vls/" rel="permalink">A reminder on Student‚Äôs t-distribution and p-values
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock" aria-hidden="true"></i> 


  
	  4 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2025-09-14T00:00:00-07:00">September 14, 2025</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>Let‚Äôs see what Sir <a href="https://www.stat.cmu.edu/~brian/valerie/617-2022/0%20-%20books/2004%20-%20wasserman%20-%20all%20of%20statistics.pdf">Larry Wasserman</a> has to tell us about it.</p>

<p>So we‚Äôre in the realm of hypothesis testing [<a href="https://stat.cmu.edu/~aramdas/icml25/ramdas1.pdf">Aaditya Ramdas</a> calls it stochastic proof by contraction, <em>I<br />
love this phrase</em>]. Suppose we divide our parameter space $\Theta$ into two disjoint sets $\Theta_0$ and $\Theta_1$.<br />
We wish to test</p>

<p>\(H_0: \theta \in \Theta_0 \quad\text{versus} \quad H_1: \theta \in \Theta_1\).</p>

<p>We call $H_0$ the null hypothesis that we‚Äôd like to reject. Because it says nothing interesting is going on <br />
(hence the name null). Let $\mathbb{P}_\theta$ be a probability distribution with support $\mathcal{X}$<br />
parameterized by $\theta$. Define $\mathcal{R} \subset \mathcal{X}$ called the rejection region.<br />
Let $X \sim \mathbb{P}$<sub>$\theta$</sub>$(\cdot)$. Then, if</p>

<p>[X \in \mathcal{R} \Rightarrow \text{ reject } H_0, \<br />
X \notin \mathcal{R} \Rightarrow \text{ retain } H_0.]</p>

<p>Usually, the rejection region is of the form</p>

<p>[R = {x \in \mathcal{X}: T(x) &gt; c },]</p>

<p>where $T$ is a  test statistic and $c$ is a  critical value. The problem in hypothesis testing is to find an appropriate  test statistic $T$ and an appropriate critical value $c$.</p>

<p>P.S.: A test statistic is  a single number calculated from sample data that is used to evaluate a hypothesis in statistical analysis. It quantifies the difference between the observed data and what would be expected if the null hypothesis were true. Essentially, it helps determine how compatible your data is with a specific hypothesis.</p>

<p><strong>Definition</strong>: The power function of a test with rejection region $\mathcal{R}$ is defined by</p>

<p>[\beta(\theta) = \mathbb{P}_\theta(X \in \mathcal{R}).]</p>

<p>The size of a test is defined to be</p>

<p>[\sup_{\theta \in \Theta_0} \beta(\theta).]</p>

<p>A test is said to have level $\alpha$ if its size is less than or equal to $\alpha$.<br />
Basically, the level $\alpha$ specifies the <strong>maximum</strong> probability of rejection the null hypothesis.</p>

</p>
    

    

  </article>
</div>

  
  
  





<div class="list__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://alirezakazemipour.github.io/posts/2025/09/lnr-algbr/" rel="permalink">Some reminders on Linear Algebra
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock" aria-hidden="true"></i> 


  
	  2 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2025-09-13T00:00:00-07:00">September 13, 2025</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>In this post I‚Äôll mention a handful reminders of Linear Algebra.</p>

</p>
    

    

  </article>
</div>

  
  
  





<div class="list__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://alirezakazemipour.github.io/posts/2025/09/indctn-by-cntrdctn/" rel="permalink">Induction by Contradiction
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock" aria-hidden="true"></i> 


  
	  1 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2025-09-12T00:00:00-07:00">September 12, 2025</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>In this post, we wanna prove <em>the correctness of the proof by induction itself</em>. Lol</p>

</p>
    

    

  </article>
</div>

  
  
  





<div class="list__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://alirezakazemipour.github.io/posts/2025/09/dtrmnstc-mrkv-plcy/" rel="permalink">When is a deterministic optimal policy in MDPs attainable?
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock" aria-hidden="true"></i> 


  
	  10 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2025-09-10T00:00:00-07:00">September 10, 2025</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>In non-theory research papers, I often see that the optimal (deterministic Markov) policy for MDPs is defined as</p>

</p>
    

    

  </article>
</div>

  
  
  





<div class="list__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://alirezakazemipour.github.io/posts/2025/09/stchstc-apprxmtn-1/" rel="permalink">Stochastic Approximation Part 1
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock" aria-hidden="true"></i> 


  
	  26 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2025-09-07T00:00:00-07:00">September 07, 2025</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>What struck my curiosity to investigate why $Q$-learning and SARSA converge was the realization that these methods use <br />
their estimates of action values at time step $t$ to estimate the action values at time step $t + 1$ in their update<br />
targets. This sounded really weired to me as though the convergence should not happen. So, I dug in and discovered the <br />
answer lies in the topic of stochastic approximation. Stochastic approximation is fairly a big topic, hence I cover it<br />
in four separate parts. In the last part I‚Äôll turn my attention to Q-Learning and SARSA eventually.</p>

<p>I‚Äôll mention the assumption required to each proposition during their corresponding proofs to see where those<br />
assumptions were inevitably needed.</p>

<h1 id="needed-background">Needed background</h1>
<p>In this section I‚Äôll review some concepts needed throughout the document.</p>

</p>
    

    

  </article>
</div>

  
  
  





<div class="list__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://alirezakazemipour.github.io/posts/2025/08/ltv/" rel="permalink">The law of total variance in practice
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock" aria-hidden="true"></i> 


  
	  8 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2025-08-19T00:00:00-07:00">August 19, 2025</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>In this post, I‚Äôll solve an example that requires the use the law of total variance in RL.</p>

<h1 id="background">Background</h1>
<p>Background on some technical tools used in the main section.  Before we start, note that for two random variables $X\, \mathrm{and}\, Y$, $\mathbb{E}[Y \mid X]$ is a shorthand notation for $\mathbb{E}[Y \mid \sigma(X)]$!</p>

<h2 id="the-law-of-total-variance">The law of total variance</h2>
<p>Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, $\mathcal{G}_1 \subseteq \mathcal{G}_2 \subseteq \mathcal{F}$ two sub-$\sigma$-algebras of $\mathcal{F}$, $X$ an integrable random variable on $(\Omega, \mathcal{F}, \mathbb{P})$  with finite variance. The following holds true:</p>

<p>\(\mathbb{V}(X \mid \mathcal{G}_1) = \mathbb{\mathbb{E}}[\mathbb{V}(X \mid \mathcal{G}_2) \mid \mathcal{G}_1] - \mathbb{\mathbb{V}}[\mathbb{E}(X \mid \mathcal{G}_2) \mid \mathcal{G}_1].\)</p>
<h2 id="the-law-of-the-unconscious-statistician-lotus">The law of the unconscious statistician (LOTUS)</h2>
<p>Let $\mathbb{P}_X$ be the pushforward of the random element $X \in \mathcal{X}$. For any real-valued, $f: \mathcal{X} \to \mathbb{R}$ measurable function,</p>

<p>[\mathbb{E}[f(X)] = \sum_xf(x)\mathbb{P}_X(x),]</p>

<p>or</p>

<p>[\mathbb{E}[f(X)] = \int_\mathcal{X} f(x)\mathrm{d}\mathbb{P}_X(x),]</p>

<p>provided that either the right-hand side, or the left-hand side exist. This is known as the ‚Äúlaw  of  the  unconscious  statistician‚Äù, or LOTUS.</p>

<h2 id="jensens-inequality">Jensen‚Äôs inequality</h2>

<p>Let $\bar{\mathbb{R}} = \mathbb{R} \cup$ <a href="2025-08-19-ltv.md">2025-08-19-ltv.md</a><a href="2025-08-19-ltv.md">2025-08-19-ltv.md</a>{$ -\infty, +\infty$}, and $\mathrm{dom}(f) = {x \in \mathbb{R}^d: f(x) &lt; \infty }$ for a real-valued function $f$ on $\mathbb{R}^d$.</p>

<p><em>Jensen‚Äôs inequality</em>: Let $f: \mathbb{R}^d \to \bar{\mathbb{R}}$ be a measurable convex function and $X$ be an $\mathbb{R}^d$-valued random element on some probability space such that $\mathbb{E}[X]$ exists and $X \in \mathrm{dom}(f)$ holds almost surely. Then,</p>

<p>[\mathbb{E}[f(X)] \geq f(\mathbb{E}[X]).]</p>

<h1 id="an-example-the-law-of-total-variance-in-practice">An example the law of total variance in practice</h1>
<p>We want to compare the variance of the target for SARSA and expected SARSA. The update rule of is</p>

<p>[Q_{t + 1}(S_t, A_t) = Q_t(S_t, A_t) + \alpha \left[R_{t + 1} + \gamma Q_t(S_{t + 1}, A_{t + 1}) - Q_t(S_t, A_t) \right].]</p>

<p>And the update rule for expected SARSA is</p>

<p>[Q_{t + 1}(S_t, A_t) = Q_t(S_t, A_t) + \alpha \left[R_{t + 1} + \gamma \sum_{a‚Äô \in \mathcal{A}} \pi\left(a‚Äô \mid S_{t + 1}\right)Q_t \left(S_{t + 1}, a‚Äô \right) - Q_t(S_t, A_t) \right],]</p>

<p>where $\pi$ is the fixed policy that was used to generate the data $S_0, A_0, R_1, \, \dots\;$.</p>

<p>Let $H_t = (S_0, A_0, R_1, \dots, S_t)$, and $H‚Äô_t = (S_0, A_0, R_1, \dots, S_t, A_t)$.</p>

<ol>
  <li>Show that</li>
</ol>

<p>[\mathbb{V}\left[Q_t(S_{t + 1}, A_{t + 1}) \middle\vert H_{t + 1}\right] \geq \mathbb{V}\left[\sum_{a‚Äô \in \mathcal{A}} \pi\left(a‚Äô \mid S_{t + 1}\right)Q_t \left(S_{t + 1}, a‚Äô \right) \middle\vert H_{t + 1}\right].]</p>

<p>First, note that using Markov property, we can replace $H_{t + 1}$ in the above expressions with $S_{t + 1}$. Second,</p>

<p>[\mathbb{V}\left[\sum_{a‚Äô \in \mathcal{A}} \pi\left(a‚Äô \mid S_{t + 1}\right)Q_t \left(S_{t + 1}, a‚Äô \right) \middle\vert S_{t + 1}\right] = 0.]</p>

<p>Because given $S_{t + 1}$ (it is not random anymore), the randomness in the actions is averaged out using the expectation, so there is no randomness remaining more. Third,</p>

<p>[\begin{align<em>}<br />
\mathbb{V}\left[Q_t(S_{t + 1}, A_{t + 1}) \middle\vert S_{t + 1}\right] &amp; = \mathbb{E}\left[Q_t(S_{t + 1}, A_{t + 1})^2 \middle\vert S_{t + 1}\right] - \mathbb{E}\left[Q_t(S_{t + 1}, A_{t + 1}) \middle\vert S_{t + 1}\right]^2 \quad \text{(def of variance)} \<br />
&amp; = \sum_{a \in \mathcal{A}} \pi(a \mid S_{t + 1}) Q_t(a, S_{t + 1})^2 - \left(\sum_{a \in \mathcal{A}} \pi(a \mid S_{t + 1}) Q_t(a, S_{t + 1}) \right)^2 \quad \text{(def of expectation and LOTUS)} \<br />
&amp;\geq \sum_{a \in \mathcal{A}} \pi(a \mid S_{t + 1}) Q_t(a, S_{t + 1})^2 - \sum_{a \in \mathcal{A}} \pi(a \mid S_{t + 1})^2 Q_t(a, S_{t + 1})^2 \quad \text{(Cauchy-Swhartz)} \<br />
&amp;\geq \sum_{a \in \mathcal{A}} \pi(a \mid S_{t + 1}) Q_t(a, S_{t + 1})^2 - \sum_{a \in \mathcal{A}} \pi(a \mid S_{t + 1}) Q_t(a, S_{t + 1})^2 \<br />
&amp;\geq 0.<br />
\end{align</em>}]</p>

<p>Right off the bat, we already knew that the variance is always non-negative anyway. üòÖ</p>
<ol>
  <li>When would have the equality happened?</li>
</ol>

<p>Well you‚Äôre asking when the variance of a random variable is zero. Then, the answer is when it‚Äôs deterministic. For a deterministic random variable $X$ we have that $\mathbb{E}[X] = X$. Hence, we want</p>

<p>[\mathbb{E}\left[Q_t(S_{t + 1}, A_{t + 1}) \middle\vert S_{t + 1}\right] = Q_t(S_{t + 1}, A_{t + 1}),]</p>

<p>which can only happen when the policy is deterministic.</p>

<ol>
  <li>Show that</li>
</ol>

<p>[\mathbb{V} \left[R_{t + 1} + \gamma Q_t(S_{t + 1}, A_{t + 1}) \middle\vert H‚Äô<em>t \right] \geq \mathbb{V}\left[R</em>{t + 1} + \gamma \sum_{a‚Äô \in \mathcal{A}} \pi\left(a‚Äô \mid S_{t + 1}\right)Q_t \left(S_{t + 1}, a‚Äô \right) \middle\vert H‚Äô_t\right],]</p>

<p>that is, the appropriate conditional variance of the SARSA target is always at least as large as that of for the expected SARSA target.</p>

<p>For convenience, let $Z_t = (S_t, A_t)$. We have,</p>

<p>[\begin{align<em>}<br />
&amp; \mathbb{V} \left[R_{t + 1} + \gamma Q_t(S_{t + 1}, A_{t + 1}) \middle\vert H‚Äô_t \right] - \mathbb{V}\left[R_{t + 1} + \gamma \sum_{a‚Äô \in \mathcal{A}} \pi\left(a‚Äô \mid S_{t + 1}\right)Q_t \left(S_{t + 1}, a‚Äô \right) \middle\vert H‚Äô_t\right] =  \<br />
&amp; \mathbb{V} [R_{t + 1} \mid Z_t] + \gamma^2 \mathbb{V} \left[Q_t(S_{t + 1}, A_{t + 1}) \middle\vert Z_t \right] + 2\mathrm{Cov}\left[R_{t + 1},  \gamma Q_t(S_{t + 1}, A_{t + 1}) \middle\vert Z_t \right] - \mathbb{V} [R_{t + 1} \mid Z_t] - \gamma^2 \mathbb{V}\left[\sum_{a‚Äô \in \mathcal{A}} \pi\left(a‚Äô \mid S_{t + 1}\right)Q_t \left(S_{t + 1}, a‚Äô \right) \middle\vert Z_t\right]  - 2\mathrm{Cov}\left[R_{t + 1},  \gamma \sum_{a‚Äô \in \mathcal{A}} \pi\left(a‚Äô \mid S_{t + 1}\right)Q_t \left(S_{t + 1}, a‚Äô \right)  \middle\vert Z_t \right]= \<br />
&amp; \gamma^2 \mathbb{V} \left[Q_t(S_{t + 1}, A_{t + 1}) \middle\vert Z_t \right] + 2\mathrm{Cov}\left[R_{t + 1},  \gamma Q_t(S_{t + 1}, A_{t + 1}) \middle\vert Z_t \right]  - \gamma^2 \mathbb{V}\left[\sum_{a‚Äô \in \mathcal{A}} \pi\left(a‚Äô \mid S_{t + 1}\right)Q_t \left(S_{t + 1}, a‚Äô \right) \middle\vert Z_t\right]  - 2\mathrm{Cov}\left[R_{t + 1},  \gamma \sum_{a‚Äô \in \mathcal{A}} \pi\left(a‚Äô \mid S_{t + 1}\right)Q_t \left(S_{t + 1}, a‚Äô \right)  \middle\vert Z_t \right].<br />
\end{align</em>}]</p>

<p>For covariance terms, note that although $R_{t + 1}$ is not independent of $Q_t$ but given $Z_t$, it is (it is naturally independent of $S_{t + 1}$ and $A_{t + 1}$). So, covariance terms are zero. Hence, we end up with</p>

<p>[\mathbb{V} \left[R_{t + 1} + \gamma Q_t(S_{t + 1}, A_{t + 1}) \middle\vert H‚Äô<em>t \right] - \mathbb{V}\left[R</em>{t + 1} + \gamma \sum_{a‚Äô \in \mathcal{A}} \pi\left(a‚Äô \mid S_{t + 1}\right)Q_t \left(S_{t + 1}, a‚Äô \right) \middle\vert H‚Äô<em>t\right] = <br />
\gamma^2 \left(\mathbb{V} \left[Q_t(S</em>{t + 1}, A_{t + 1}) \middle\vert Z_t \right]   - \mathbb{V}\left[\sum_{a‚Äô \in \mathcal{A}} \pi\left(a‚Äô \mid S_{t + 1}\right)Q_t \left(S_{t + 1}, a‚Äô \right) \middle\vert Z_t\right]\right).]</p>

<p>Now we apply the law of total variance</p>

<p>[\begin{align<em>}<br />
&amp;\gamma^2 \left(\mathbb{V} \left[Q_t(S_{t + 1}, A_{t + 1}) \middle\vert Z_t \right]   - \mathbb{V}\left[\sum_{a‚Äô \in \mathcal{A}} \pi\left(a‚Äô \mid S_{t + 1}\right)Q_t \left(S_{t + 1}, a‚Äô \right) \middle\vert Z_t\right]\right) = \<br />
&amp;\gamma^2 \left(\mathbb{E}\left[\mathbb{V}\left[Q_t(S_{t + 1}, A_{t + 1}) \middle\vert S_{t + 1} \right] \middle\vert Z_t \right]   -  \mathbb{V}\left[\mathbb{E}\left[Q_t(S_{t + 1}, A_{t + 1}) \middle\vert S_{t + 1} \right] \middle\vert Z_t \right] \right) - \<br />
&amp;\gamma^2 \left( \mathbb{E}\left[\mathbb{V}\left[\sum_{a‚Äô \in \mathcal{A}} \pi\left(a‚Äô \mid S_{t + 1}\right)Q_t \left(S_{t + 1}, a‚Äô \right) \middle\vert S_{t + 1}\right] \middle\vert Z_t \right] - \mathbb{V}\left[\mathbb{E}\left[\sum_{a‚Äô \in \mathcal{A}} \pi\left(a‚Äô \mid S_{t + 1}\right)Q_t \left(S_{t + 1}, a‚Äô \right) \middle\vert S_{t + 1}\right] \middle\vert Z_t \right] \right)<br />
\end{align</em>}]</p>

<p>First, $\mathbb{E}\left[\mathbb{V}\left[\sum_{a‚Äô \in \mathcal{A}} \pi\left(a‚Äô \mid S_{t + 1}\right)Q_t \left(S_{t + 1}, a‚Äô \right) \middle\vert S_{t + 1} \right] \middle\vert Z_t \right] = 0$. Given $S_{t + 1}$ everything inside becomes deterministic. Also,</p>

<p>[\mathbb{V}\left[\mathbb{E}\left[Q_t(S_{t + 1}, A_{t + 1}) \middle\vert S_{t + 1} \right] \middle\vert Z_t \right]  = \mathbb{V}\left[ \sum_{a‚Äô} \pi(a‚Äô \mid S_{t + 1})Q_t(S_{t + 1}, a‚Äô) \middle\vert Z_t \right]= \mathbb{V}\left[\mathbb{E}\left[ \sum_{a‚Äô} \pi(a‚Äô \mid S_{t + 1})Q_t(S_{t + 1}, a‚Äô) \middle\vert S_{t + 1}\right] \middle\vert Z_t \right]]</p>

<p>So, we have</p>

<p>[\mathbb{V} \left[R_{t + 1} + \gamma Q_t(S_{t + 1}, A_{t + 1}) \middle\vert H‚Äô<em>t \right] - \mathbb{V}\left[R</em>{t + 1} + \gamma \sum_{a‚Äô \in \mathcal{A}} \pi\left(a‚Äô \mid S_{t + 1}\right)Q_t \left(S_{t + 1}, a‚Äô \right) \middle\vert H‚Äô<em>t\right] =  \gamma^2 \mathbb{E}[\mathbb{V}[Q_t(S</em>{t + 1}, A_{t + 1}) \mid S_{t + 1}] \mid Z_t] \geq 0.]</p>

<p>Since the variance is always positive.</p>

<h1 id="reference">Reference</h1>

<ul>
  <li><a href="https://szepi.github.io/cmput-365-w25/">CMPUT 365: Introduction to Reinforcement Learning</a>,  <a href="https://szepi.github.io/cmput-365-w25/documents/worksheets/w7_soln.pdf">Worksheet 7</a>.</li>
  <li><a href="https://sites.ualberta.ca/~szepesva/books.html">Bandit Algorithms</a>.</li>
</ul>
</p>
    

    

  </article>
</div>

  
  
  





<div class="list__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://alirezakazemipour.github.io/posts/2025/08/sblv/" rel="permalink">Sobolev spaces (not too wiggly functions), Hilbert spaces, and RHKS
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock" aria-hidden="true"></i> 


  
	  11 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2025-08-17T00:00:00-07:00">August 17, 2025</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>This post will be hell of a post! Haha ü§©</p>

</p>
    

    

  </article>
</div>

  
  
  





<div class="list__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://alirezakazemipour.github.io/posts/2025/08/mmnts/" rel="permalink">Random variables‚Äô moments and thereof
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock" aria-hidden="true"></i> 


  
	  5 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2025-08-17T00:00:00-07:00">August 17, 2025</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>Mostly from <a href="https://www.stat.cmu.edu/~brian/valerie/617-2022/0%20-%20books/2004%20-%20wasserman%20-%20all%20of%20statistics.pdf">All of Statistics</a>.</p>

</p>
    

    

  </article>
</div>

  
  
  





<div class="list__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://alirezakazemipour.github.io/posts/2025/08/rlc/" rel="permalink">What to remember from RLC 2025
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock" aria-hidden="true"></i> 


  
	  3 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2025-08-06T00:00:00-07:00">August 06, 2025</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><h1 id="day-1">Day 1:</h1>

</p>
    

    

  </article>
</div>

  
  
  





<div class="list__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://alirezakazemipour.github.io/posts/2025/08/icml/" rel="permalink">What to remember from ICML 2025
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock" aria-hidden="true"></i> 


  
	  less than 1 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2025-08-06T00:00:00-07:00">August 06, 2025</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><p>Andreas Krause‚Äôs first step in research (keynote talk):</p>
<blockquote>
  <p>You have to know what you don‚Äôt know.</p>
</blockquote>

</p>
    

    

  </article>
</div>

  
  
    <h2 id="1900" class="archive__subtitle">1900</h2>
    
  
  





<div class="list__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://alirezakazemipour.github.io/posts/1900/08/qlrng-sarsa/" rel="permalink">Why does Q-Learning and SARSA converge? (a.k.a Stochastic Approximation Part 4)
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock" aria-hidden="true"></i> 


  
	  1 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="1900-08-20T00:00:00-08:00">August 20, 1900</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><h1 id="in-progress-">In Progress!!! üëá</h1>

</p>
    

    

  </article>
</div>

  </div>
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/alirezakazemipour"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <!-- <li><a href="https://alirezakazemipour.github.io/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li> -->
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 Alireza Kazemipour. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="https://alirezakazemipour.github.io/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>






  </body>
</html>

